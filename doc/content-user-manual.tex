\include{header}
\newcommand{\version}{0.1}
\newcommand{\documentname}{\refusermanual}

\begin{document}

\include{titlepage}

\renewcommand{\contentsname}{Contents}
\tableofcontents
\bigskip

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{chap:introduction}

The advent of Open Data\footnote{\url{http://opendatahandbook.org/}} and Linked Data\footnote{\url{http://www.w3.org/standards/semanticweb/data}; \url{http://linkeddata.org/}}  accelerates the evolution of the Web into an exponentially growing information space\footnote{See the Linked Open Data Cloud at \url{http://richard.cyganiak.de/2007/10/lod/}} where the unprecedented volume of data will offer information consumers a~level of information integration and aggregation agility that has up to now not been possible. Data consumers can now \quot{mashup} and readily integrate information in myriads of applications.

Indiscriminate addition of information, however, comes with inherent problems, such as the provision of poor quality, inaccurate, irrelevant or fraudulent information. All will come with an associate cost of the data integration which will ultimately affect data consumer's benefit and Linked Data applications usage and uptake.

To overcome these issues, we present a~framework enabling management of Linked Data -- data cleaning, linking, transformation and quality assessment -- and providing  applications with a~possibility to consume the stored cleaned and integrated data, which reduces the costs of application development.

\section{How to Read This Document}

This document is a~user manual with basic description od ODCleanStore and detailed instructions on how to access and work with the application from the perspective of a~user. Chapters \ref{chap:introduction} and \ref{chap:howItWorks} give a~basic description of what ODCleanStore is and how it works, while Chapter \ref{chap:userRoles} describes user roles and will guide you to other parts of this manual relevant for your user role.

If more detailed information is needed, please refer to related documents \quot{\refadmimanual} and \quot{\refprogrammersguide}.

\todo{what is important, what is different from others, how do we support the goals}

\section{What is ODCleanStore}

In short, ODCleanStore is a~server application that stores RDF data, processes them and provides integrated views on the data.

ODCleanStore accepts arbitrary RDF data through a~webservice (together with provenance and other metadata). The data is processed by \term{transformers} in one of a~set of customizable \term{pipelines} and stored to a~persistent store. The stored data can be accessed again through a~webservice. Linked Data consumers can send queries and custom query policies to this webservice and receive (aggregated/integrated) RDF data relevant for their query, together with information about provenance and data quality.

ODCleanStore is developed at the Charles University in Prague, Faculty of Mathematics and Physics as part of the \href{http://opendata.cz}{OpenData.cz} initiative and the \href{http://lod2.eu}{LOD2.eu} project and published as a~free software under Apache License 2.0. The project is hosted at SourceForge at
\begin{center}
  \url{http://sourceforge.net/p/odcleanstore/}.
\end{center}



\section{Linked Data Framework}

The goal of the \href{http://opendata.cz}{OpenData.cz} initiative is to build an open data infrastructure in The Czech Republic. It would provide public data in a~form that allows access to anyone at any time and allows to combine it freely. This would allow the creation of applications that the public really needs.

ODCleanStore is a~part of the Linked Data Framework developed under the OpenData.cz initiative. The main three parts of the framework are \term{Data Acquisition} module, \term{Data Aggregation and Cleaning} module and \term{Data Visualization and Analysis} module.

\todo{picture}

The Data Acquisition module\footnote{\url{http://strigil.sourceforge.net/}} will be able to crawl webpages and scrape structured data from webpages and other sources (such as XLS spreadsheets). This data is converted to RDF and sent to the Data Aggregation and Cleaning module represented by ODCleanStore. ODCleanStore processes the data, stores it and provides access to it. The Visualization and Analysis module will query ODCleanStore and provide a~human-friendly interface to end users.


\section{Examples of Deployment}

ODCleanStore is planned to be deployed together with the Data Acquisition module represented by project Strigil\footnotemark[\thefootnote] which would feed up-to-date data to ODCleanStore. However, thanks to the use of standard formats for communication with the input/output webservices, ODCleanStore can be deployed with any other third-party application for data feeding or consuming.

In general, ODCleanStore is intendend to be used whenever there are multiple sources of (semi-)structured data convertible to RDF that need to be integrated. ODCleanStore can be used for academic purposes, \quot{mashup} applications, or even deployed in an enterprise environment.

A real-world deployment is planned for storing public contracts data published by the public administration of the Czech Republic as part of the OpenData.cz initiative. Another deployment will be for internal use in students' projects at the Charles University in Prague.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{How It Works}
\label{chap:howItWorks}

ODCleanStore consists of \term{Engine}, \term{Input Webservice} and \term{Output Webservice} (both run as part of the Engine), and administration webfrontend. The Engine processes incoming and stored data using \term{transformers}. A~transformer is a~pluggable Java class implementing a~defined interface; several transformers ship with ODCleanStore, such as Quality Assessment, Object Identification or Data Normalization.

\todo{picture, EA diagramy}

\section{Data Lifecycle}

The lifecycle of data inside ODCleanStore is as follows:

\begin{enumerate}
  \item RDF data (and additional metadata) is accepted by Input Webservice and stored as a~named graph to the \term{dirty database}. Data can be uploaded by any third-party application registered in ODCleanStore.
  \item Engine successively processes named graphs in the dirty database by applying a~pipeline of transformers to it; the applied pipeline is selected according to the input metadata.
  \item Each transformer in the pipeline may modify the named graph or attach new related named graphs (such as a named graph with mappings to other resources or results of quality assessment).
  \item When the pipeline finishes, the augmented RDF data are populated to the \term{clean database} together with any auxiliary data and metadata created during the pipeline execution.
  \item Data consumers can use Output Webservice to query data in the clean database. Output Webservice provides several basic types of queries -- URI query, keyword and named graph query; in addition, metadata about a~given named graph can be requested. The response to a~query consists of relevant RDF triples together with their provenance information and quality estimate. The query can be further customized by user-defined conflict resolution policies.\\
	Data in the clean database can be also queried using the SPARQL query language. While SPARQL queries are more expressive, there is no direct support for provenance tracking and quality estimation. 
  \item When transformer rules change, the administrator may choose to re-run a~pipeline on data already stored in the clean database. Copy of this data is created in the dirty database where it is processed by the pipeline. After that, the processed version of data replaces the original in the clean database.
\end{enumerate}

\section{Administration Frontend Features}
The administration webfrontend enables
\begin{itemize}
  \item management of user accounts,
  \item management of pipelines, transformers and transformer rules,
  \item management of ontologies,
  \item monitoring of the state of Engine,
  \item management of other settings, such as default conflict resolution policies for queries.  
\end{itemize}

\section{Summary of Features}

\begin{itemize}
  \item Administration in a~simple web interface.
  \item Input and Output Webservices communicate in standard formats - Input Webservice accepts RDF/XML or TTL, Output Webservice provides results in HTML, TriG and RDF/XML formats.
  \item Highly customizable pipelines for incoming data processing. Different pipelines can be used for different data sources.
  \item Data can be processed before they are stored to a~persistent store but also when they are already stored if neccessary.
  \item Ships with several predefined transformers for use in data-processing pipelines: Data Normalization (transformations of data), Quality Assesment (estimates quality of data based on a~set of rules), Object Identification (links RDF resources representing the same entity or otherwise related). All these transformers can be managed in the web administration interface.
  \item Support for ontology management. Mappings between ontologies can be defined in order to integrate heterogeneous data. Also, rules for  transformers can be automatically generated from ontologies.
  \item Data consumers can query for all data about a~given resource or use the keyword search.
  \item Response to a~query includes provenance information and quality estimate of each RDF triple in the result. More provenance metadata can be requested. Conflicts that arise when integrating data are solved at query time according to user-defined policies. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{User Roles}
\label{chap:userRoles}

\todo{use-case diagram}

Data consumers accessing Output Webservice (see Section~\ref{sec:outputWS}) do not need to have an account in ODCleanStore; these users have a~special role User (USR). Other users working with ODCleanStore need to have an account and their permissions are based on the roles they are assigned. This chapter describes all the roles recognized by ODCleanStore.

\section[Administrator]{Administrator (ADM)}
\label{sec:adm}

	Administrator has privileges to manage user accounts, assign roles and manage system-wide settings such as
	\begin{itemize}
		\item transformers that can be used in pipelines created by pipeline creators,
		\item settings of Output Webservice (default aggregation policies, etc.),
		\item URI prefixes that can be used in settings and queries.
	\end{itemize}

	In addition, the administrator is authorized to edit pipelines and rules created by pipeline creators.

% TODO Tohle bych dal do admin manualu, ma to na starost administrator ODCS jako takovy, coz neni nutne uzivatel s roli ADM:
%	The administrator is responsible for ensuring that not more than one instance of the storage and its web frontend is connected to one instance of database.


	More information, e.g. about adding transformers, can be found in the related document \refadmimanual.

	\paragraph{Most relevant sections of this document:} Chapter \ref{chap:administrationFrontend} \nameref{chap:administrationFrontend}.

\section[Ontology Creator]{Ontology Creator (ONC)}
\label{sec:onc}
	The ontology creator can import and edit ontologies registered in the system. The ontology creator is also responsible for inserting mappings (\code{owl:sameAs} links) between ontologies.

	\paragraph{Most relevant sections of this document:} Section \ref{sec:ontologyManagement} \nameref{sec:ontologyManagement}.

\section[Pipeline Creator]{Pipeline Creator (PIC)}
\label{sec:pic}
	The pipeline creator can create input data processing pipelines. This includes creating new pipelines and assigning transformers to them (Section \ref{sec:pipelineManagement}) and also creating rules for the transformers (Section \ref{sec:transformerRules}).

	Every pipeline creator is allowed to create custom pipelines and rule groups for predefined transformers. The pipeline creator has a~read-only access to other creators' pipelines and rules (and can use such rules in custom pipelines), however rules and pipelines can only be edited by their author. The only exception is the administrator, who can edit arbitrary pipelines and rule groups.

	\paragraph{Most relevant sections of this document:} Sections \ref{sec:pipelineManagement} \nameref{sec:pipelineManagement},\linebreak[4] \ref{sec:transformerRules} \nameref{sec:transformerRules} and \ref{sec:pipelinesOnCleanDB} \nameref{sec:pipelinesOnCleanDB}.
	
\section[Data Producer]{Data Producer (SCR)} \todo{SCR? DAP?}
\label{sec:scr}
  The data producer can use Input Webservice (Section \ref{sec:inputWS}) to insert new data to ODCleanStore. The system keeps track of which data were inserted by which data producer.

  \paragraph{Most relevant sections of this document:} Sections \ref{sec:inputWS} \nameref{sec:inputWS} and \linebreak[4] \ref{sec:inputProcessing}~\nameref{sec:inputProcessing}.

\section[Data Consumer]{Data Consumer (USR)} \todo{USR? DAC?}
\label{sec:usr}
  The data consumer can use Output Webservice (Section \ref{sec:outputWS}) to ask queries over the data in the clean database. This role is special in that users in this role do not need to have an account (any user using the Output Webservice is automatically assigned the USR role).

  \paragraph{Most relevant sections of this document:} Sections \ref{sec:outputWS} \nameref{sec:outputWS} and \linebreak[4] \ref{sec:storedDataStructure}~\nameref{sec:storedDataStructure}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Administration Frontend}
\label{chap:administrationFrontend}

\section{Administration Frontend Overview}

Administration Frontend is the official tool for managing ODCleanStore. It covers configuration of all standard components.

Several terms and designations are used repeatedly in the frontend, however, their meanings do not change too much, therefore make sure to be familiar with them as they might not be described hereafter.

\subsection*{Attributes}

\enumtable
{
	label & A unique human readable identifier of the related entity\\
	description & A description for user's purposes and better comprehension of semantics of the related entity
}

\subsection*{Actions}

\enumtable
{
	Edit & edit attributes of the related entity\\
	Delete & remove the related entity irrevertably from the system\\
	Detail & view entities and configurations related to the related entity\\
	Rerun affected graphs & due to a change of a configuration of a transformer (modification of a rule group) the stored data may be found in an inconsistent form, therefore it is possible for the user to identify such situations and decide to run the transformer again
}

\section{User Accounts Administration}

To be able to manage user accounts one has to have \term{ADM} user role (section \ref{sec:adm}) associated with his account. It is possible to view currently active accounts and modify them or to add new accounts to the system from this page. The modification is limited to changes of user roles and resetting of passwords for safety reasons. A newly generated password is sent to the e-mail address of the affected user.

\section{Transformer Management}
\label{sec:transformerManagement}

	\term{Transformer} is a component responsible for data refinement, cleaning, aggregation and other transformations applied to incoming or stored data.

	The transformer management screen allows registered users to add, edit or remove {transformers}. These can be then added to \term{pipelines} (Section \ref{sec:pipelineManagement}).
	
	Each {transformer} definition consists of:

	\fieldtable
	{
		required & label & \\
		optional & description & \\
		required & JAR path & Path to a Java Archive containing the {transformer} declaration.\\
		required & Full classname & Name of the class implementing the {transformer}.
	}

\section{Pipeline management}
\label{sec:pipelineManagement}

	New incoming data (in form of a named graph) accepted by Input Webservice are passed through a {pipeline} consisting of {transformers}. In this section of the administration frontend it is possible for the user to specify different {pipelines}. Individual {pipelines} can incorporate different already existing {transformers}. To edit the structure of a {pipeline}, view its detail. As the importance of data modification that the pipelines can cause differs based on what database it is running upon, it is left for the user to decide whether a concrete {transformer} should be allowed to run on clean database (in addition to running on dirty database). Some transformations do not make sense when working with clean database.
	
	The order of application of {transformers} to a named graph processed by the related {pipeline} is determined by its \term{priority} in ascending order. Every {transformer} runs before {transformers} with greater priority number.

\paragraph{Important note:} Any changes done in this section need to be commited to ever take effect. To do so, please, click the \quot{Commit settings} button.

\section{Transformer rules}
\label{sec:transformerRules}

	There are a few types of {transformers} predefined for most common data handling in pipelines. Namely:
	\begin{itemize}
		\item \term{Quality Assessessment} {transformer} (Section \ref{sub:qualityAssessment})
		\item \term{Data Normalization} {transformer}  (Section \ref{sub:dataNormalization})
		\item \term{Object Identification} {transformer} (Section \ref{sub:objectIdentification})
	\end{itemize}
	
	These {transformers} are configured through groups of rules. Each instance of any of these predefined {transformers} can accept multiple groups of rules. That way it is possible to simply assign all interrelated rules to a certain instance of a transformer while it is still possible to avoid duplication of rules in different groups.

\subsection{Quality Assessment}
\label{sub:qualityAssessment}

\term{Quality Assessor} is a special {transformer} that assigns a score to each graph based on coefficients of different patterns present in the graph to reflect to what degree the data contained in it comply to a certain policy.

To be able to configure individual instances of {Quality Assessor} a group of rules needs to exist. To create one enter the QA subsection of the rules management page for the backend.

Here the user can prepare groups of rules to be assigned to instances of {Quality Assessor}. Each group is identified by its label and can (and should) come with a description of its semantical significance.

On the detail page, one can specify individual rules contained in the related group. Each rule consists of a \code{GroupGraphPattern}\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/\#rGroupGraphPattern}} filter, quality decrease coefficient and description, as described in Table \ref{tbl:qaFields}.

\begin{table}[!h]
	\centering
	\begin{tabular}{rl@{\hspace{8mm}}c@{\hspace{8mm}}c}
		& \textbf{Filter} & \textbf{Coefficient} & \textbf{Description} \\
		& \code{GroupGraphPattern} [\code{GROUP BY \ldots} [\code{HAVING \ldots}]] & $ x \in [0, 1] $ & description \\
		\\
		ex: & \code{\{\{?s anatomy:limbs ?o\} FILTER (?o > 4)\}} & $ 0.4 $ & To many limbs
	\end{tabular}
	\caption{Quality Assessment rule fields}
	\label{tbl:qaFields}
\end{table}

Any snippet of SPARQL\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/#grammar}} to which \quot{\code{SELECT * FROM ... WHERE}} can be prepended is a valid filter and describes a property of a named graph that the author of the rule finds defective.
%%TODO: mention/omit: The use of SPARQL 1.1 features depends on underlaying Virtuoso OpenSource version.

\subsection{Data Normalization}
\label{sub:dataNormalization}

\term{Data Normalizer} is a special type of {transformer} aimed to be applied early in the whole data evaluation process to simplify work of other {transformers}. Its main goal is to remove inconsistencies in forms the data is provided in. Therefore {Data Normalizer} instances are limited to dirty database only.

In the DN subsection of rules management page of the backend, one can prepare groups of rules to be assigned to instances of {Data Normalizer}. Each group is identified by its label and can (and should) come with a description of its semantical significance.

The detail page of a group serves the user as means of specification of individual rules contained in the selected group. Each rule is essentially a sequence of \term{SPARUL}\footnote{\url{http://www.w3.org/Submission/SPARQL-Update}} \term{Inserts} and/or \term{Deletes}. New rule represents an empty sequence upon its creation.

Similarly as with the rules themselves the detail page of a rule allows the user to construct any arbitrary sequence of {Inserts} and/or {Deletes}. Components of the rule (members of the sequence) can be added by specifying their type (either \code{INSERT} or \code{DELETE}), modification ({SPARUL} snippet stripped off of initial \code{INSERT INTO} / \code{DELETE FROM} clauses); e.g.,

\begin{center}
	\code{\{?s ?p1 ?o2\} WHERE \{GRAPH \$\$graph\$\$ \{?s ?p1 ?o1. ?o1 ?p2 ?o2.\}\}},
\end{center}

where \code{\$\$graph\$\$} is a place holder for name of the graph being currently processed. Expectedly triples (\code{?s ?p1 ?o2} in the example) are inserted into (deleted from) the current graph when the type of the component is \code{INSERT} (\code{DELETE}). Effects are immediate in respect to consecutive applications of other components of the same rule or other rules to the graph.

%%TODO: notice: use of GRAPH $$graph$$ is highly recommended in cases the other is not sure he tries to avoid restriction to the processed graph only

\subsection{Object Identification}
\label{sub:objectIdentification}

\term{Object Identification Linker} is a special implementation of a {trasformer}. The main purpose of this component is to interlink URIs which represent the same real-world entity by generating \code{owl:sameAs} links. It can be also used for creating other types of links between differently related URIs. {Silk framework}\footnote{\url{http://www4.wiwiss.fu-berlin.de/bizer/silk}} is used as the linking engine. Sets of linkage rules for the engine are written in {Silk-LSL}\footnote{\url{http://www.assembla.com/wiki/show/silk/Link\_Specification\_Language}} and stored in database.

\section{Ontology Management}
\label{sec:ontologyManagement}

Ontologies can be used to produce common rules for QA, DN {transformers}. To load one into the storage one can provide an explicit definition through a text field or by uploading a file containing a valid {RDF/XML} ontology definition. The process of rules generation will automatically take place upon ontology submission.

\section{Other Settings}

\subsection{Query Execution \& Conflict Resolution}
\label{sec:QEnCR}

\subsection{Prefix management}
\label{sec:frontendPrefixMgmt}

To avoid obligation of full manual URI expansion in \term{transformer} rules or queries it is possible to maintain set of global \term{RDF} prefixes that storage recognizes.

\section{Configuration Example}

In this section a basic concept of ODCleanStore configuration will be illustrated.

{
%%TODO: choose one:
%%After the installation of the storage there will be a user account \emph{adm} with password \emph{adm} allowing the user to access the administration account in its full potential. It is necessary to log into the system and it is advisable to change users email address and then reset the password right away.

It is necessary to log into the frontend with credentials given during the installation. All of the following operations will be possible to be done with the initial user account.
}

There need to be transformers for the storage to be able to handle incoming data. ODCleanStore comes with its built in transformers that are accessible from the frontend right after the installation. Custom transformers need to be added at this point.

Add the ODCSPropertyFilterTransformer by following steps:

\begin{enumerate}
	\item[] \textbf{Prepare transformer}
	\item Choose \code{Backend/Transformers} from the frontend menu
	\item Click \quot{Add a new transformer}
	\item Fill in label of your choice:
	
	ODCSPropertyFilterTransformer
	
	\item Describe its purpose:
	
	Removes reserved properties from the processed graph
	
	\item Select the path to the backend JAR:
	
	.
	
	\item Fill in the classname:
	
	cz.cuni.mff.odcleanstore.transformer.odcs.ODCSPropertyFilterTransformer
	
	\item[]\item[] \textbf{Prepare rules for standard transformer}
	\item Choose \code{Backend/Rules/DN} from the frontend menu
	\item Click \quot{Add a new group}
	\item Fill in necessary information and submit it
	\item Click \quot{Detail} in the row of the group created in the previous step
	\item Click \quot{Add a new rule}, fill in its description and submit
	\item Click \quot{Detail} in the row of the rule created in the previous step
	\item Click \quot{Add a new rule component}
	\item Choose the type of the data transformation (INSERT/DELETE)
	\item Specify the triples that will modify the graph
	
	ex: \code{\{?s prefix1:property1 ?o\} WHERE \{?s prefix2:property2 ?o\}}
	
	\item Describe the meaning of this transformation and submit it
	\item Repeat until the rule is complete
	
	\item[]\item[] \textbf{Prepare pipeline}
	
	%%TODO: backend/pipelines hazi chybu
	
	%%TODO: nahrani dat? odkaz na scraper?
	
	%%TODO: debugging ... neni jeste hotov

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Web Services}
\section{Web Services Overview}
ODCleanStore communicates with third-party applications via webservices. Data producers can store data to ODCleanStore through \term{Input Webservice}, while data consumers may use \term{Output Webservice} to query the stored \& processed data. In addition, stored data can be accessed through a~public SPARQL endpoint. Input Webservice requires authorization, Output Webservice and the SPARQL endpoint do not.

\section{Data Producer}
\label{sec:inputWS}

New data can be stored to ODCleanStore through Input Webservice, a~SOAP multithreaded webservice that accepts RDF data serialized as RDF/XML\footnote{\url{http://www.w3.org/TR/rdf-syntax-grammar/}} or TTL\footnote{TTL or Turtle -- Terse RDF Triple Language; \url{http://www.w3.org/TeamSubmission/turtle/}} and additional metadata. The webservice requires authorization with a~valid user name and password.

The location of Input Webservice can be configured by the \code{input\_ws.endpoint\_url} configuration option (see \refadmimanual); by default, it is:
\begin{center}
\varcode{host}\code{:8080/inputws}
\end{center}

See Section \ref{sec:inputProcessing} for more information about how the inserted data are processed and stored.

\subsection{Request parameters}
\label{sec:inputWSParams}

Table \ref{tbl:inputWSParams} enumerates parameters of Input Webservice. All the parameters are required.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
	\hline
	Name & Description & Type \\
	\hline \hline
	\code{user} & user login name & \vartext{string}  \\
	\hline
	\code{password} & user password  & \vartext{string} \\
	\hline
	\code{payload} & data to insert serialized as RDF/XML or TTL & \vartext{string} \\
	\hline
	\code{metadata} & metadata about \code{payload} & see Table \ref{tbl:inputWSMetadata} \\
	\hline
\end{tabular}
\caption{Input Webservice parameters}
\label{tbl:inputWSParams}
\end{table}

\subsubsection{Metadata}
Table \ref{tbl:inputWSMetadata} lists fields that the \code{metadata} parameter consists of.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{|l|X|p{2.2cm}|c|}
	\hline
	Name & Description & Type & Cardinality\\
	\hline \hline
	\code{uuid} & UUID string unique for the current request & UUID & 1 \\
	\hline
	\code{dataBaseUrl} & base URI for resolution of relative URIs in \code{payload} & URI & 1 \\
	\hline
	\code{source} & location of where the data were retrieved from & URI & 1..* \\
	\hline
	\code{publishedBy} & identifier(s) of the publisher(s) of the data & URI & 1..* \\
	\hline
	\code{license} & license(s) under which the data are published & URI & 0..* \\
	\hline
	\code{provenance} & additional provenance metadata serialized as RDF/XML or TTL & RDF/XML\newline or TTL & 0..1 \\
    \hline
	\code{pipelineName} & identifier of the pipeline that should process the inserted data & \vartext{string} & 0..1  \\
	\hline
\end{tabularx}
\caption{Input Webservice \code{metadata} fields}
\label{tbl:inputWSMetadata}
\end{table}

Each request is identified by a~unique UUID generated on the client side and sent in the \code{uuid} field.
The client side is responsible for generating different UUIDs for new requests.
UUID doesn't change during the whole message transfer nor in case of a~repeated request after an exception.
\todo{PJ: popsat proč a jak; doplnit popis odolnosti vůči vícenásobným uuid}

The \code{dataBaseUrl} field is the base URI for \code{payload}.

The \code{source} field is a~list or URIs the data were retrieved from. Typically, this would be URI(s) of webpage(s) the data were scraped from but in general it can be any URI.

The \code{publishedBy} field is a~list of URIs representing the publisher of the data. It can be a~well known URI, or, for example, the host part of the source URI (e.g. \linebreak[4]\code{http://en.wikipedia.org/} for data scraped from the English Wikipedia).

The \code{license} field may specify URI(s) representing the license(s) under which \code{payload} contents and any additional  metadata being inserted are published. 

The optional \code{provenance} field can contain additional RDF provenance metadata about contents of \code{payload}.\footnote{The suggested vocabulary for these purposes is W3P (\url{http://code.google.com/p/od-w3p/}).} Base URI for the provenance metadata is the URI of the named graph where \code{payload} is stored in ODCleanStore.

The optional \code{pipelineName} field can contain a~string identifier of an existing pipeline in ODCleanStore that should be used to process the inserted data. If omitted, the default pipeline is used. \todo{reference na příslušnou sekci Admin manual}

\subsection{Exceptions}
\todo{PJ: nejsem si jistý, jak u WS výjimky fungují -> upravit, aby to odpovídalo skutečnosti, doplnit a učesat}

Error during a~request to Input Webservice is indicated by throwing an exception. Table \ref{tbl:inputWSExceptions} summarizes exceptions that can occur. In case of such exception, no data or \code{uuid} value for the interrupted request are stored.

\todo{PJ: jak se indikuje úspěch?}

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{|l|c|X|}
	\hline
	Exception & Code & Description \\
	\hline \hline
	SERVICE\_BUSY & 1 & Service busy -- occurs when maximum limit of concurrent connections is exceeded \\
	\hline
	BAD\_CREDENTIALS & 2 & Bad credentials (invalid \code{user} or \code{password}) \\
	\hline
	NOT\_AUTHORIZED & 3 & Not authorized -- user doesn't have SCR role \mbox{assigned} \\
	\hline
	DUPLICATED\_UUID & 4 & Duplicate uuid -- another request with the same \code{uuid} value has already successfully finished\\
	\hline
	UUID\_BAD\_FORMAT & 5 & Wrong format of the \code{uuid} field\\
	\hline
	UNKNOWN\_PIPELINENAME & 6 & No pipeline with name as given in \code{pipelineName} \mbox{exists} \\
	\hline
	OTHER\_ERROR & 7 & Other error; when a~new transmission with the same \code{uuid} as the current \code{uuid} is started before the current transmission finishes, OTHER\_ERROR is thrown; only the new transmission will continue \\
	\hline
	FATAL\_ERROR & 8 & Fatal error\\
	\hline
	METADATA\_ERROR & 9 & Invalid metadata -- a~field has a~wrong format or a~required field is missing \\
	\hline
\end{tabularx}
\caption{Input Webservice exceptions}
\label{tbl:inputWSExceptions}
\end{table}

\FloatBarrier

\subsection{Java API}
Third-party applications can access Input Webservice directly, or use the Java client library provided in ODCleanStore distribution. \todo{location on the CD}
Add \code{odcs-inputclient-}\textit{version}\code{.jar} library to your project and use class \code{OdcsService} to access Input Webservice programmatically.

\lstlistingname~\ref{lst:clientLibrary} gives an example of how the client library can be used.

\begin{lstlisting}[caption={Example usage of Input Webservice client library},label=lst:clientLibrary]
try {
  final int BUFFER_SIZE = 1024 * 4;
  char[] buffer = new char[BUFFER_SIZE];
  int count = 0;

  StringBuilder  payload = new StringBuilder();
  InputStreamReader payloadReader = new InputStreamReader(
          new FileInputStream("data.rdf"), "UTF-8");
  while (-1 != (count = payloadReader.read(buffer, 0, BUFFER_SIZE))) {
      payload.append(buffer, 0, count);
  }
  payloadReader.close();

  StringBuilder  provenancePayload = new StringBuilder();
  InputStreamReader provenanceReader = new InputStreamReader(
          new FileInputStream("provenance-metadata.rdf"), "UTF-8");
  while (-1 != (count = provenanceReader.read(buffer, 0, BUFFER_SIZE))) {
      provenancePayload.append(buffer, 0, count);
  }
  provenancePayload.close();


  Metadata metadata = new Metadata();
  metadata.setUuid(UUID.randomUUID());
  metadata.setDataBaseUrl(new URI("http://en.wikipedia.org/wiki/Berlin"));
  metadata.getSource().add(new URI("http://en.wikipedia.org/wiki/Berlin"));
  metadata.getPublishedBy().add(new URI("http://en.wikipedia.org"));
  metadata.getLicense().add(new URI("http://creativecommons.org/licenses/by-sa/3.0/")); 
  metadata.setPipelineName("examplePipeline"); 
  metadata.setProvenance(provenancePayload.toString()); 

  OdcsService service = new OdcsService("http://localhost:8088/inputws");
  service.insert("username", "password", metadata, payload.toString());
} catch (Exception e) {
  e.printStackTrace();
}
\end{lstlisting}

\section{Data Consumer}
\label{sec:outputWS}

A consumer of data stored in ODCleanStore can query the database through Output Webservice. The Output Webservice can be queried for data about a~given URI resource, queried by keywords, queried for contents of a~given named graphs or queried for metadata of a~named graph. Conflicts in data returned in response to a~query are resolved and the data are fused using policies provided by the user or by the administrator.

Additionally, the user can access the data in the clean database directly using the SPARQL endpoint powered by Virtuoso.\footnote{\url{http://virtuoso.openlinksw.com/}} This way the data consumer can use the full power of the SPARQL query language, however conflict resolution and provenance tracking is not supported for this type of queries.

\todo{specify where the SPARQL endpoint can be accessed, link to Virtuoso documentation; mention it is read-only}

\section*{Output Webservice}

The Output Webservice is a~REST webservice which can be accessed using both GET and POST HTTP methods equivalently. The port where the webservice resides can be configured by the \code{output\_ws.port} configuration option (see \refadmimanual); by default, it is on port \code{8087}.

\subsection{Types of queries}

The Output Webservice can be queried for:

\begin{enumerate}
	\item a~resource URI -- \term{URI query}
	\item keyword(s) -- \term{keyword query}
	\item named graph contents -- \term{named graph query}
	\item named graph metadata -- \term{metadata query}
\end{enumerate}

Table \ref{tbl:queryTypes} lists where each type of query can be accessed by default. The exact address can be configured.

\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{|l|l|X|}
	\hline
	Query & URI & Example of a~query \\
	\hline \hline
	URI & \varcode{host}\code{/uri} & \mbox{http://localhost:8087/uri} \mbox{?uri=http\%3A\%2F\%2Fexample.com} \\
	\hline
	Keyword & \varcode{host}\code{/keyword} & http://localhost:8087/keyword?kw=keyword\\
	\hline
	Named graph & \varcode{host}\code{/namedGraph} & http://localhost:8087/namedGraph \mbox{?uri=http\%3A\%2F\%2Fexample.com}\\
	\hline
	Metadata & \varcode{host}\code{/metadata} & \mbox{http://localhost:8087/metadata} \mbox{?uri=http\%3A\%2F\%2Fexample.com} \\
	\hline
\end{tabularx}
\caption{Types of queries}
\label{tbl:queryTypes}
\end{table} 

More information is available in the Query Execution specification. \todo{reference}

\subsection{Request format}

Table \ref{tbl:requestFormatUK} lists (either GET or POST) parameters than can be used with the URI, keyword and named graph queries. The \code{uri} parameter is required for URI query, \code{kw} parameter for keyword query. Other parameters are optional.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{|l|X|p{2cm}|p{2cm}|}
	\hline
	Name & Description & Possible values & Default value \\
	\hline \hline
	\code{uri} & searched URI; \newline \textit{used only with URI and named graph query} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{kw} & searched keyword(s); \newline \textit{used only with keyword query} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{format} & format of the response & \code{html}, \code{trig}, \code{rdfxml} & \code{html} \\
	\hline
	\code{aggr} & default aggregation method & \vartext{string} & \code{ALL} \\
	\hline
	\code{es} & error strategy -- handling of values for which aggregation fails & \code{IGNORE}, \code{RETURN\_ALL} & \code{RETURN\_ALL} \\
	\hline
	\code{multivalue} & default multivalue setting & 0, 1 & 0 \\
	\hline
	\code{paggr[\vartext{property}]} & aggregation method for the given \mbox{property};  example:

	  \code{paggr[rdfs\%3Alabel]=ANY} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{pmultivalue[\vartext{property}]} & multivalue setting for the given \mbox{property}; example:

	  \code{pmultivalue[rdf\%3Atype]=1} & 0, 1 & \vartext{N/A} \\
	\hline
\end{tabularx}
\caption{URI, keyword and named graph query parameters}
\label{tbl:requestFormatUK}
\end{table} 

Table \ref{tbl:requestFormatNG} lists parameters that can be used with the metadata query.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
	\hline
	Name & Description & Possible values & Default value & Required \\
	\hline \hline
	\code{uri} & URI of the requested named graph & \vartext{string} & \vartext{N/A}  & yes\\
	\hline
	\code{format} & format of the result & \code{html}, \code{trig}, \code{rdfxml} & \code{html} & no \\
	\hline
\end{tabular}
\caption{Metadata query parameters}
\label{tbl:requestFormatNG}
\end{table}

For all queries, parameters and values are case-sensitive. Property names may be either full URIs, or prefixed names (e.g. \code{rdfs:label}). Available prefixes are managed in the administration frontend (see section \ref{sec:frontendPrefixMgmt}).

For more information about aggregation settings, see the corresponding section of Conflict Resolution specification\todo{reference}.

%\newpage % just for the tables to fit better

\subsubsection*{General aggregation methods}

\enumtable
{
	ALL & returns all conflicting values \\
	BEST & value with the highest aggregated quality; in case of equality, the newest timestamp is preferred \\
	LATEST & value with the newest timestamp; in case of equality, the highest aggregate quality is preferred \\
	ANY & returns a~single arbitrary value \\
	CONCAT & concatenation of conflicting values separated by \quot{\code{;\ }} \\
	NONE & returns all conflicting values including duplicities
}

\subsubsection*{Numeric aggregation methods}

\enumtable
{
	MIN & minimum of conflicting values \\
	MAX & maximum of conflicting values \\
	AVG & average of conflicting values \\
	MEDIAN & median of conflicting values
}

\subsubsection*{Date aggregation methods}

\enumtable
{
	MIN & the earliest date \\
	MAX & the latest date
}

\subsubsection*{String aggregation methods}

\enumtable
{
	SHORTEST & the shortest string \\
	LONGEST & the longest string
}

\subsubsection*{Error strategy}
The error strategy determines how to handle values that cannot be aggregated by the given aggregation method, e.g. when applying MEDIAN aggregation to a~mix of numeric and date values.

Note that for some aggregations, an untyped literal may be converted to a~numeric literal (\code{xsd:double}) if possible.

\subsubsection*{Multivalue parameter}
The multivalue parameter determines whether differences with other conflicting values decrease quality (\code{multivalue=0}), or not (\code{multivalue=1}). Setting multivalue to false (0) is appropriate for properties with a~single value (e.g. \code{dbprop:population}), setting it to true (1) is appropriate for propertiees with multiple possible values (e.g. \code{rdf:type}).

\todo{content-negotiation}

\subsection{Query Format}

\subsubsection{URI Query}
The value of the \code{uri} parameter must be either a~full valid URI, or a~prefixed name (e.g. \code{dbpedia:Berlin}).  Available prefixes are managed in the administration frontend (see section \ref{sec:frontendPrefixMgmt}).

\subsubsection{Keyword Query}
The \code{kw} parameter can contain one or more keywords separated by whitespace. If a~keyword itself contains spaces, it may be enclosed in double quotes. Query Execution looks for literals that contain all of the keywords. Keywords can also contain the \code{*} wildcard, but they must begin with at least four non-wildcard characters if a~wildcard is to be used.

Query Execution also looks for an exact match of the entire \code{kw} value (i.e. without any division to keywords). If the \code{kw} value is a~number, then numeric typed literals will also match; if the \code{kw} value is formatted as \code{xsd:dateTime}\footnote{\url{http://www.w3.org/TR/xmlschema-2/\#dateTime-lexical-representation}}, then \code{xsd:dateTime} typed literals will also match.

\subsubsection{Named Graph Query}
The value of the \code{uri} parameter must be either a~valid URI, or a prefixed name, of an existing named graph.

\subsubsection{Metadata Query}
The value of the \code{uri} parameter must be a~valid URI of an existing named graph.

\subsection{Results Format for URI \& Keyword Queries}
\label{sec:URIKWResultsFormat}

The result contains triples returned in response to the query, including relevant labels of URI resources in the result, and metadata for the triples.

\subsubsection{HTML}

The result in HTML format contains results in a~human-readable form. It contains

\begin{itemize}
	\item a~table with all triples in the result together with their aggregated quality and named graphs from which the triple was selected or calculated,
  \item  a~table with metadata of named graphs occuring in the first table.
\end{itemize}

\subsubsection{TriG}
\label{sec:URIKWTrig}

If the \code{format} parameter is set to \code{trig}, the result contains triples (quads) serialized in the TriG\footnote{\url{http://www4.wiwiss.fu-berlin.de/bizer/trig/}} format. The result includes:

\begin{itemize}
	\item triples returned in response to the query, each one placed in a~unique named graph
  \item aggregated quality (\code{odcs:quality}) and source named graphs (\code{odcs:sourceGraph}) of the above triples; subjects of these statements are the unique named graphs where the respective triples are placed
  \item  metadata of source named graphs; they may include where the data were extracted from (\code{w3p:source}), Quality Assesment score of the named graph (\code{odcs:score}) and of its publisher (\code{odcs:publisherScore}), the publisher of the data (\code{w3p:publishedBy}), timestamp (\code{w3p:insertedAt}), license (\code{dc:license})
  \item  metadata about the query response itself -- a~title (\code{dc:title}), date (\code{dc:date}), number of result triples (\code{odcs:totalResults}), the query (\code{odcs:query}) and link to each result item (\code{odcs:result})
\end{itemize}

\pagebreak[3]

An example:

\begin{lstlisting}[caption={Example of URI or keyword query response in TriG},label=lst:URIKWTrigResponse]
@prefix :        <#> .
@prefix odcs:    <http://opendata.cz/infrastructure/odcleanstore/> .
@prefix w3p:     <http://purl.org/provenance#> .
@prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .
@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dbpedia:     <http://dbpedia.org/ontology/> .

<http://opendata.cz/infrastructure/odcleanstore/query/results/1> {
  <http://dbpedia.org/resource/Berlin> rdfs:label "Berlin"@en .
}

<http://opendata.cz/infrastructure/odcleanstore/query/results/2> {
  <http://dbpedia.org/resource/Berlin> dbpedia:populationTotal
    "3420768"^^<http://www.w3.org/2001/XMLSchema#int> .
}

<http://opendata.cz/infrastructure/odcleanstore/query/metadata/> {
  <http://opendata.cz/infrastructure/odcleanstore/query/results/1>
    odcs:quality 0.92 ;
    w3p:source <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde> ;
    w3p:source <http://opendata.cz/infrastructure/odcleanstore/data/b68e21f7-363f-4bfd> .

  <http://opendata.cz/infrastructure/odcleanstore/query/results/2>
    odcs:quality 0.8966325468133597 ;
    w3p:source <http://opendata.cz/infrastructure/odcleanstore/data/b68e21f7-363f-4bfd> .

  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    odcs:score 0.9 ;
    w3p:insertedAt "2012-04-01 12:34:56.0"^^<http://www.w3.org/2001/XMLSchema#dateTime> ;
    w3p:source <http://dbpedia.org/page/Berlin> ;
    w3p:publishedBy <http://dbpedia.org/> ;
    odcs:publisherScore 0.9 .

  <http://opendata.cz/infrastructure/odcleanstore/data/b68e21f7-363f-4bfd>
    odcs:score 0.8 ;
    w3p:insertedAt "2012-04-04 12:34:56.0"^^<http://www.w3.org/2001/XMLSchema#dateTime> ;
    w3p:source <http://linkedgeodata.org/page/node240109189> .

  <http://localhost:8087/uri?uri=http%3A%2F%2Fdbpedia.org%2Fresource%2FBerlin>
    a odcs:QueryResponse ;
    dc:title "URI search: http://dbpedia.org/resource/Berlin" ;
    dc:date "2012-08-01T10:20:30+01:00" ;
    odcs:totalResults 2 ;
    odcs:query "http://dbpedia.org/resource/Berlin" ;
    odcs:result <http://opendata.cz/infrastructure/odcleanstore/query/results/1> ;
    odcs:result <http://opendata.cz/infrastructure/odcleanstore/query/results/2> .
}
\end{lstlisting}

\subsubsection{RDF/XML}
\label{sec:URIKWRDFXML}

If the \code{format} parameter is set to \code{rdfxml}, then the result will be formatted in RDF/XML.\footnote{\url{http://www.w3.org/TR/REC-rdf-syntax/}} The returned triples contain

\begin{itemize}
  \item triples returned in response to the query,
  \item  metadata about the query response itself
\end{itemize}

as in case of TriG output, however no metadata about quality of triples or about source named graphs are included.

\subsubsection{Paging of results}
\label{sec:URIKWPaging}
As of now, all results are returned on a single page. The approximate maximum number of triples in the result is 500 by default (and can be set in the configuration file, see \refadmimanual).

\subsection{Results Format for Named Graph Query}
Named graph query selects all triples stored in the given named graph and is intended mainly for debugging purposes.
The format of results for the named graph query is exactly the same as for URI or keyword queries (see Section~\ref{sec:URIKWResultsFormat}). The only difference is that labels for URI resources in the result are not retrieved (unless they are contained in the named graph). Also, conflict resolution considers only the named graph and not any other conflicting (or same) values that may be stored in other graphs.

\subsection{Results Format for Metadata Query}
The result contains metadata and Quality Assessment results for a given named graph. The metadata include metadata maintained by ODCleanStore (e.g. \code{odcs:insertedAt}) and data from the \code{provenance} metadata graph. Quality Assessment is executed on the named graph at query time, with rules that would be applied to it in its respective pipeline.

\subsubsection{HTML}

The result in HTML format contains results in a~human-readable form. It contains

\begin{itemize}
  \item a table with ODCleanStore metadata,
  \item the results of Quality Assessment, i.e. the resulting score and all QA rules the named graph violated and thus its score was decreased by the respective coefficient (only if there is at least one QA rule group applicable to the named graph),
  \item \code{provenance} metadata, if available.
\end{itemize}


\subsubsection{TriG}
\label{sec:metadataTriG}
The result contains triples (quads) serialized in the TriG format. Again, the result contains ODCleanStore metadata, additional \code{provenance} metadata, results of Quality Assessment and also metadata about the query response itself. The meaning of used predicates is as described in Section \ref{sec:URIKWTrig}.

The \code{provenance} metadata are contained in one named graph and a triple \varcode{payload-graph}-\code{odcs:provenanceMetadataGraph}-\varcode{provenance-graph} points to it; all other data are placed in another named graph.

\pagebreak[3]

An example:

\begin{lstlisting}[caption={Example of metadata query response in TriG},label=lst:metadataTrigResponse]
@prefix :        <#> .
@prefix odcs:    <http://opendata.cz/infrastructure/odcleanstore/> .
@prefix w3p:     <http://purl.org/provenance#> .
@prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .
@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc:      <http://purl.org/dc/terms/> .

<http://opendata.cz/infrastructure/odcleanstore/query/metadata/> {
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:insertedAt "2012-04-01 12:34:56.0"^^<http://www.w3.org/2001/XMLSchema#dateTime> ;
    w3p:source <http://dbpedia.org/page/Berlin> ;
    w3p:publishedBy <http://dbpedia.org/> ;
    odcs:provenanceMetadataGraph
      <http://opendata.cz/infrastructure/odcleanstore/provenanceMetadata/e0cdc9d7-e2d8-4bde>;
        
    odcs:score 0.72 ;
    odcs:violatedQARule <http://opendata.cz/infrastructure/odcleanstore/QARule/10> ;
    odcs:violatedQARule <http://opendata.cz/infrastructure/odcleanstore/QARule/20> .  

  <http://opendata.cz/infrastructure/odcleanstore/QARule/10>
    a odcs:QARule ;
    odcs:coefficient 0.8 ;
    dc:description "Procedure type ambiguous" .
        
  <http://opendata.cz/infrastructure/odcleanstore/QARule/20>
    a odcs:QARule ;
    odcs:coefficient 0.9 ;
    dc:description "Procurement contact person missing" .
        
  <http://localhost:8087/namedGraph?uri=http%3A%2F%2Fopendata.cz
      %2Finfrastructure%2Fodcleanstore%2Fdata%2Fe0cdc9d7-e2d8-4bde>
    a odcs:QueryResponse ;
    dc:title "Metadata for named graph:
      http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde" ;
    dc:date "2012-08-01T10:20:30+01:00" ;
    odcs:query "http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde";
}
    
<http://opendata.cz/infrastructure/odcleanstore/provenanceMetadata/e0cdc9d7-e2d8-4bde> {
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:provenanceMetadataProperty1 "provenanceMetadataValue1".
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:provenanceMetadataProperty2 "provenanceMetadataValue2".
}
\end{lstlisting}

\subsubsection{RDF/XML}
The result for a metadata query serialized in RDF/XML contains the same triples as in case of TriG (Section \ref{sec:metadataTriG}) except that triples are not divided into named graphs.

\subsubsection{Paging of results}
See Section~\ref{sec:URIKWPaging}.

\todo{JSON}
\todo{Error handling!}
\todo{Examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Stored Data}
\section{Input Processing}
\label{sec:inputProcessing}
\todo{PJ: zkontrolovat, jestli odpovida realite, a naopak, ze vse potrebne je implementovano}
When a new request is sent to Input Webservice, the stored data \& metadata go through several phases.

\begin{enumerate}

  \item First, data \& metadata are validated. \code{Payload} data and optional \code{provenance} metadata (see Section~\ref{sec:inputWSParams} \nameref{sec:inputWSParams}) should be valid RDF/XML or TTL, all required metadata fields must have the proper cardinality and valid format. An exception is thrown and the request interrupted if validation fails.

  \item If all data are valid, the request is queued, Input Webservice indicates success and the transmission successfully finishes.

  \item Engine, independently on Input Webservice, successively takes requests from the input queue and processes them. RDF data from \code{payload} are stored to a single named graph, \code{provenance} metadata to a separate named graph and other metadata to another separate named graph called \term{metadata graph}, all in the dirty (staging) database. The format of RDF triples in the metadata graph is described in Section~\ref{sec:storedDataStructure}.

  \item Because some predicates are reserved for purposes of internal metadata representation in ODCleanStore, RDF triples that contain these predicates are removed from \code{payload} and \code{provenance} named graphs. Table~\ref{tbl:reservedPredicates} lists all reserved predicates.

\begin{table}[h!]
\centering
\begin{tabular}{|l|}
	\hline 
	\code{odcs:score} \\
	\hline
	\code{odcs:publisherScore} \\
	\hline
	\code{odcs:scoreTrace} \\
	\hline
	\code{odcs:metadataGraph} \\
	\hline
	\code{odcs:provenanceMetadataGraph} \\
	\hline
	\code{odcs:sourceGraph} \\
	\hline
	\code{odcs:insertedAt} \\
	\hline
	\code{odcs:insertedBy} \\
	\hline
	\code{odcs:source} \\
	\hline
	\code{odcs:publishedBy} \\
	\hline
	\code{odcs:license} \\
	\hline
\end{tabular}
\caption{Reserved RDF predicates}
\label{tbl:reservedPredicates}
\end{table}
\todo{check it is up-to-date}

  \item Next, the processing pipeline is selected -- if \code{pipelineName} was present, pipeline with the given name is used, the default pipeline is used otherwise. Engine runs each transformer in the pipeline on the stored data. Transformers can modify the inserted named graphs or attach new named graphs (\term{attached named graph}). See~\refadmimanual{} for more information about transformers.

  \item If all transformers in the pipeline finish successfully, the \code{payload} graph, \code{provenance} graph, metadata graph and any new attached graphs are moved from the dirty database to the clean database, while the respective request is removed from the queue.

  \todo{popsat, co se deje, kdyz nastane chyba}

\end{enumerate}

\section{Stored Data Structure}
\label{sec:storedDataStructure}

Data originating from a single request to Input Webservice can be stored in several named graphs. RDF data given in the \code{payload} parameter are stored in one named graph (\code{payload} graph). If \code{provenance} RDF metadata are given, they are stored in another named graph (\code{provenance} graph). Other metadata (such as the source of data, timestamp, etc.) are stored in yet another named graph (\term{metadata graph}). In addition, transformers in the respective pipeline may add more related RDF data to one or more named graphs (\term{attached graphs}), e.g. results of quality assessment, or mappings for resources in \code{payload}. 

While contents of the \code{payload}, \code{provenance} and attached graphs may be arbitrary, the metadata graph has a set structure. Table~\ref{tbl:metadataGraph} describes the structure of a metadata graph. In the table, \varcode{payload-graph} stands for the name of the respective \code{payload} graph, \varcode{provenance-graph} and \varcode{metadata-graph} analogously.


\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{|l|p{5cm}|X|c|}
	\hline
	Subject & Predicate & Object & Cardinality\\
	\hline \hline
	\varcode{payload-graph} & \code{odcs:metadataGraph} & \varcode{metadata-graph} & 1 \\
	\hline
	\varcode{payload-graph} & \code{odcs:} \code{provenanceMetadataGraph} & \varcode{provenance-graph} & 0..1 \\
	\hline
	\varcode{payload-graph} & \code{odcs:attachedGraph} & URIs of attached graphs & 0..* \\
	\hline
	\varcode{payload-graph} & \code{odcs:insertedAt} & insertion time & 1 \\
	\hline
	\varcode{payload-graph} & \code{odcs:insertedBy} & name of the user who inserted the data & 1 \\ \todo{PJ: jmeno nebo ID?}
	\hline
	\varcode{payload-graph} & \code{odcs:source} & source of the data (values  from the \code{source} field) & 1..* \\
	\hline
	\varcode{payload-graph} & \code{odcs:publishedBy} & identifier of the publisher of the data (values  from the \code{publishedBy} field) & 1..* \\
	\hline
	\varcode{payload-graph} & \code{odcs:license} & license of the data (values from the \code{license} field) & 0..* \\
	\hline
\end{tabularx}
\caption{RDF triples in a metadata graph}
\label{tbl:metadataGraph}
\end{table}

Note that transformers may add triples to the metadata graph too. For example, Quality Assessment adds these two triples:
\begin{itemize}
  \item \varcode{payload-graph} -- \code{odcs:score} -- \varcode{QA-score}
  \item \varcode{payload-graph} -- \code{odcs:scoreTrace} -- \varcode{QA-score-explanation}
\end{itemize}

\section{Executing Pipelines on the Clean Database}
\label{sec:pipelinesOnCleanDB}

The administrator \todo{proper user role} can decide to re-run a transformer pipeline on named graphs that are already in the clean database, e.g. when the respective transformer rules changed.\todo{reference to administration documentation} In that case, all such named graphs are queued for processing and Engine successively runs the pipeline on each queued graph:

\begin{enumerate}
  \item First, a copy of the \code{payload}, \code{provenance}, metadata and any attached graphs is created in the dirty database.
  \item The same processing pipeline that was used when the data came through Input Webservice is run on this copy. Transformers can modify any of the graphs and attach new graphs.
	\todo{what if the pipeline fails?}
  \item In a transaction, the old version in the clean database is deleted and the processed copy (together with any new attached graphs) is moved from the dirty database to the clean database.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\chapter{Glossary}

\section*{RDF-related}
\begin{glossarylist}
	\item[RDF] Resource Description Framework, a~language for representing information about resources in the World Wide Web\footnote{\url{http://www.w3.org/RDF/}}
	\item[RDF triple] Statement about a~resource expressed in the form of subject-predicate-object expression
	\item[URI] Uniform Resource Identifier, identifies RDF resources
	\item[Named graph] A~set of related RDF triples (RDF graph) named with a~URI\footnote{\url{http://www.w3.org/2004/03/trix/}}
	\item[RDF quad] An RDF triple plus named graph URI (subject, predicate, object, named graph)
	\item[Ontology] Representation of the meaning of terms in a~vocabulary and of their interrelationships
	\item[OWL] The Web Ontology Language\footnote{\url{http://www.w3.org/TR/owl-features/}}
	\item[SPARQL] RDF query language\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/}}
	\item[RDF/XML] An XML-based serialization format for RDF graphs\footnote{\url{http://www.w3.org/TR/rdf-syntax-grammar/}}
	\item[TTL] Turtle -- Terse RDF Triple Language\footnote{\url{http://www.w3.org/TeamSubmission/turtle/}}; a~human-friendly alternative to RDF/XML 
\end{glossarylist}

\section*{Data \& Data Quality}
\begin{glossarylist}
	\item[Dirty (staging) database] Database where incoming data are stored until they are processed by a~processing pipeline (e.g. clean, linked to other data, etc.)
	\item[Clean database] Database where incoming data are stored after they are successfully processed by the respective processing pipeline; this database can be accessed using the Output Webservice
	\item[\code{Payload} graph] Named graph where the actual inserted data, given in the \code{payload} parameter of Input Webservice, are stored
	\item[\code{Provenance} graph] Named graph where additional provenance metadata, given in the \code{provenance} field of Input Web Service, are stored
	\item[Metadata graph] Named graph where other metadata about a \code{payload} graph (such as source, timestamp, license, etc.) are stored
	\item[Attached graph] Named graph attached to a \code{payload} graph by a transformer
	\item[Named graph score] Quality of a~single (\code{payload}) named graph estimated by the Quality Assesment component and stored in the database, expressed as a~number from interval [0,1]
	\item[Publisher score] Average score of named graphs from a~publisher
	\item[Aggregate quality] Quality of a~triple in the results calculated by the Conflict Resolution component during query time, expressed as a~number from interval [0,1]
\end{glossarylist}

\section*{User Roles}
\begin{glossarylist}
	\item[ADM] Administrator
	\item[ONC] Ontology creator
	\item[PIC] Pipeline creator
	\item[SCR] Data producer (scraper)
	\item[USR] Data consumer
\end{glossarylist}

\chapter{List of Used XML Namespaces}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	\ttfamily
	\textrm{\textbf{Prefix}} & \textrm{\textbf{URI}} \\
	\hline\hline
	odcs & http://opendata.cz/infrastructure/odcleanstore/ \\
	\hline
	w3p & http://purl.org/provenance\# \\
	\hline
	dc & http://purl.org/dc/terms/ \\
	\hline
	rdf & http://www.w3.org/1999/02/22-rdf-syntax-ns\# \\
	\hline
	rdfs & http://www.w3.org/2000/01/rdf-schema\# \\
	\hline
	owl & http://www.w3.org/2002/07/owl\# \\
	\hline
	xsd & http://www.w3.org/2001/XMLSchema\# \\
	\hline
	dbpedia & http://dbpedia.org/resource/ \\
	\hline
	dbprop & http://dbpedia.org/property/ \\
	\hline
\end{tabular}
\caption{List of used XML namespaces}
\end{table} 

\end{document}
