\include{header}
\newcommand{\version}{0.1}
\newcommand{\documentname}{User Manual}

\begin{document}

\include{titlepage}

\renewcommand{\contentsname}{Contents}
\tableofcontents
\bigskip

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

The advent of Open Data\footnote{\url{http://opendatahandbook.org/}} and Linked Data\footnote{\url{http://www.w3.org/standards/semanticweb/data}, \url{http://linkeddata.org/}}  accelerates the evolution of the Web into an exponentially growing information space (see the Linked Open Data Cloud\footnote{\url{http://richard.cyganiak.de/2007/10/lod/}}) where the unprecedented volume of data will offer information consumers a level of information integration and aggregation agility that has up to now not been possible. Data consumers can now \quot{mashup} and readily integrate information in myriads of applications.

Indiscriminate addition of information, however, comes with inherent problems, such as the provision of poor quality, inaccurate, irrelevant or fraudulent information. All will come with an associate cost of the data integration which will ultimately affect data consumer's benefit and Linked Data applications usage and uptake.

To overcome these issues, we present a framework enabling management of Linked Data -- data cleaning, linking, transformation and quality assessment -- and providing  applications with a possibility to consume the stored cleaned and integrated data, which reduces the costs of application development.

% TODO
%what is ...
%	Open data, problems, ...
%overview
%	components, how it works, features
% https://sourceforge.net/p/odcleanstore/wiki/Project%20description/
% EA diagrams

% TODO what is imporntant, what is different from others, how do we support the goals

\section{What is ODCleanStore}

ODCleanStore is a server application that stores RDF data, processes them and provides integrated views on the data.

ODCleanStore accepts arbitrary RDF data through a webservice (together with provenance and other metadata). The data is processed by \term{transformers} in one of a set of customizable \term{pipelines} and stored to a persistent store. The stored data can be accessed via another webservice. Linked Data consumers can send queries and custom query policies to this webservice and receive (aggregated/integrated) RDF data relevant for their query, together with information about provenance and data quality.

ODCleanStore is developed at the Charles University in Prague, Faculty of Mathematics and Physics as part of the \href{http://opendata.cz}{OpenData.cz} initiative and the \href{http://lod2.eu}{LOD2.eu} project and published as a free software under Apache License 2.0. The project is hosted at SourceForge at
\begin{center}
  \url{http://sourceforge.net/p/odcleanstore/}.
\end{center}



\section{Linked Data Framework}

The goal of the \href{http://opendata.cz}{OpenData.cz} initiative is to build an open data infrastructure in The Czech Republic. It would provide public data in a form that allows access to anyone at any time and allows to combine it freely. This would allow the creation of applications that the public really needs.

ODCleanStore is a part of the Linked Data Framework developed under the OpenData.cz initiative. The main three parts of the framework are \term{Data Acquisition} module, \term{Data Aggregation and Cleaning} module and \term{Data Visualization and Analysis} module.

\todo{obrázek}

The Data Acquisition module\footnote{\url{http://strigil.sourceforge.net/}} will be able to crawl webpages and scrape structured data from the webpages and other sources (such as XLS spreadsheets). This data is converted to RDF and sent to the Data Aggregation and Cleaning module represented by ODCleanStore. ODCleanStore processes the data, stores it and provides access to it. The Visualization and Analysis module will query ODCleanStore and provide a human-friendly interface to end users.


\section{Examples of Usage}

TODO: Use cases, mention expected deployment (public contracts, students).


\section{How to Read This Document}

This document is a user manual with detailed instructions on how to access and work with the application from the perspective of a user. Chapter \ref{chap:howItWorks} gives a basic description of how ODCleanStore works, while Chapter \ref{chap:userRoles} describes user roles and will guide you to other parts of this manual relevant for your user role.

If more detailed information is needed, please refer to related documents \quot{Administrator's \& Installation Manual} and \quot{Programmer's Guide}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{How It Works}
\label{chap:howItWorks}

ODCleanStore consists of \term{Engine}, \term{Input Webservice} and \term{Output Webservice} (both run as part of the Engine), and administration webfrontend. The Engine processes incoming and stored data using \term{transformers}. A transformer is a pluggable Java class implementing a defined interface; several transformers ship with ODCleanStore, such as Quality Assessment, Object Identification or Data Normalization.

\todo{obrázek}

\section{Data Lifecycle}

The lifecycle of data inside ODCleanStore is as follows:

\begin{enumerate}
  \item RDF data (and additional metadata) is accepted by Input Webservice and stored as a single named graph to the \term{dirty database}. Data can be uploaded by any third-party application registered in ODCleanStore.
  \item Engine successively processes named graphs in the dirty database by applying a pipeline of transformers to it; the applied pipeline may be specified by the input metadata.
  \item Each transformer in the pipeline may modify the named graph or attach new related named graphs (such as named graph with mappings to other resources or results of quality assessment).
  \item When the pipeline finishes, the augmented RDF data are populated to the \term{clean database} together with any auxiliary data and metadata created during the pipeline execution.
  \item Data consumers can use Output Webservice to query data in the clean database. Output Webservice provides two basic types of queries -- URI query and keyword query -- and additionally metadata about a given named graph can be requested. The response to a query consists of relevant RDF triples together with their provenance information and quality estimate. The query can be further customized by user-defined conflict resolution policies.\\
	Data in the clean database can be also queried using the SPARQL query language. While SPARQL queries are more expressive, there is no direct support for provenance tracking and quality estimation. 
  \item When rules transformer rules change, the administrator may choose to re-run a pipeline on data already stored in the clean database. Copy of this data is created in the dirty database where it is processed by the pipeline. After that the processed version od data replaces the original in the clean database.
\end{enumerate}

\section{Administration Frontend Features}
The administration webfrontend enables
\begin{itemize}
  \item management of user accounts,
  \item management of pipelines, transformers and transformer rules,
  \item management of ontologies,
  \item monitoring of the state of Engine,
  \item management of other settings, such as default conflict resolution policies.  
\end{itemize}

\section{Summary of Features}
\todo{vyhodit?}

\begin{itemize}
  \item Administration in a simple web interface.
  \item Webservices communicate in standard formats - Input Webservice accepts RDF/XML or TTL, Output Webservice provides results in the TriG format. \todo{taky RDF/XML}
  \item Highly customizable pipelines for incoming data processing. Different pipelines can be used for different data sources.
  \item Data can be processed before they are stored to a persistent store but also when they are already stored if neccessary.
  \item Ships with several predefined transformers for use in data-processing pipelines: Data Normalization (transformations of data), Quality Assesment (estimates quality of data based on a set of rules), Object Identification (links RDF resources representing the same entity or otherwise related). All these transformers can be managed in the web administration interface.
  \item Support for ontology management. Mappings between ontologies can be defined in order to integrate heterogeneous data. Also, rules for data-processing transformers can be automatically generated from ontologies.
  \item Data consumers can query for all data about a given resource or use the keyword search.
  \item Response to a query includes provenance information and quality estimate of each RDF triple in the result. More provenance metadata can be requested.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{User Roles}
\label{chap:userRoles}

\todo{use-case diagram}

Data consumers accessing the Output Webservice (see Section \ref{sec:outputWS}) do not need to have an account in ODCleanStore; these users have a special role User (USR). Other users working with ODCleanStore need to have an account and their permissions are based on the roles they are assigned. This chapter describes all the roles recognized by ODCleanStore.

\section[Administrator]{Administrator (ADM)}

	Administrator has privileges to manages user accounts, assign roles and manage system-wide settings such as
	\begin{itemize}
		\item transformers that can be used in pipelines created by pipeline creators,
		\item settings of the Output Webservice (default aggregation policies, etc.),
		\item URI prefixes that can be used in settings and queries.
	\end{itemize}

	In addition, the administrator is authorized to edit pipelines and rules created by pipeline creators.

	More information, e.g. about adding transformers, can be found in the related document Administrator's \& Installation Manual.

	\paragraph{Most relevant sections of this document:} Chapter \ref{chap:administrationFrontend} \nameref{chap:administrationFrontend}.

\section[Ontology Creator]{Ontology Creator (ONC)}
	The ontology creator can import and edit ontologies registered in the system. The ontology creator is also responsible for inserting mappings (\code{owl:sameAs} links) between ontologies.

	\paragraph{Most relevant sections of this document:} Section \ref{sec:ontologyManagement} \nameref{sec:ontologyManagement}.

\section[Pipeline Creator]{Pipeline Creator (PIC)}
	The pipeline creator can create input data processing pipelines. This includes creating new pipelines and assigning transformers to them (Section \ref{sec:pipelineManagement}) and also creating rules for the transformers (Section \ref{sec:transformerRules}).

	Every pipeline creator is allowed to create custom pipelines and rule groups for predefined transformers. The pipeline creator has a read-only access to other creators' pipelines and rules (and can use such rules in custom pipelines), however rules and pipelines can only be edited by their author. The only exception is the administrator, who can edit arbitrary pipelines and rule groups.

	\paragraph{Most relevant sections of this document:} Sections \ref{sec:pipelineManagement} \nameref{sec:pipelineManagement} and \ref{sec:transformerRules} \nameref{sec:transformerRules}.
	
\section[Data Producer]{Data Producer (SCR)} \todo{SCR? DAP?}
  The data producer can use the Input Webservice (Section \ref{sec:inputWS}) to insert new data to ODCleanStore. The system keeps track of which data were inserted by which data producer.

  \paragraph{Most relevant sections of this document:} Section \ref{sec:inputWS} \nameref{sec:inputWS}.

\section[Data Consumer]{Data Consumer (USR)} \todo{USR? DAC?}
  The data consumer can use the Output Webservice (Section \ref{sec:outputWS}) to ask queries over the data in the clean database. This role is special in that users in this role do not need to have an account (any user using the Output Webservice is automatically assigned the USR role).

  \paragraph{Most relevant sections of this document:} Section \ref{sec:outputWS} \nameref{sec:outputWS}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Administration Frontend}
\label{chap:administrationFrontend}

\section{Administration Frontend Overview}

\section{User Accounts Administration}

\section{Transformer Management}
\label{sec:transformerManagement}

\section{Pipeline Management}
\label{sec:pipelineManagement}

\section{Transformer Rules}
\label{sec:transformerRules}

\subsection*{Quality Assessment}

\subsection*{Data Normalization}

\subsection*{Object Identification}

\section{Ontology Management}
\label{sec:ontologyManagement}

\section{Other Settings}

\subsection{Query Execution \& Conflict Resolution}
\label{sec:QEnCR}

\subsection{Prefix management}
\label{sec:frontendPrefixMgmt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Web Services}

\section{Web Services Overview}

\section{Data Publisher}
\label{sec:inputWS}

\section{Data Consumer}
\label{sec:outputWS}

A consumer of data stored in ODCleanStore can query the database through the \term{Output Webservice}. The Output Webservice can be queried for data about a given URI resource, queried by keywords or queried for metadata of a given named graph. Conflicts in data returned in response to a query are resolved and the data are fused using policies provided by the user or by the administrator.

Additionally, the user can access the data in the clean database directly using the SPARQL endpoint powered by Virtuoso. This way the data consumer can use the full power of the SPARQL query language, however conflict resolution and provenance tracking is not supported for this type of queries.
\todo{define clean database (in glossary?)}
\todo{specify where the SPARQL endpoint can be accessed}

\section*{Output Webservice}

The Output Webservice is a REST webservice which can be accessed using both GET and POST HTTP methods equivalently.

\subsection{Types of queries}

The Output Webservice can be queried for:

\begin{enumerate}
	\item a resource URI
	\item keyword(s)
	\item named graph metadata
\end{enumerate}

Table \ref{tbl:queryTypes} lists where each type of query can be accessed.

\begin{table}[h]
\centering
\label{tbl:queryTypes}
\begin{tabular}{|l|l|p{7.5cm}|}
	\hline
	Query & URI & Example of a query \\
	\hline \hline
	URI & \varcode{host}\code{/uri} & \mbox{http://localhost:8087/uri} \mbox{?uri=http\%3A\%2F\%2Fexample.com} \\
	\hline
	Keyword & \varcode{host}\code{/keyword} & http://localhost:8087/keyword=keyword\\
	\hline
	Named graph metadata & \varcode{host}\code{/namedGraph} & \mbox{http://localhost:8087/namedGraph} \mbox{?uri=http\%3A\%2F\%2Fexample.com} \\
	\hline
\end{tabular}
\caption{Types of queries}
\end{table} 

More information is available in the Query Execition specification. \todo{reference}

\subsection{Request format}

The following table lists (either GET or POST) parameters than can be used with the URI and keyword queries:

\begin{table}[h!]
\centering
\label{tbl:requestFormatUK}
\begin{tabularx}{\textwidth}{|l|X|p{2cm}|p{2cm}|}
	\hline
	Name & Description & Possible values & Default value \\
	\hline \hline
	\code{uri} & searched URI; \newline \textit{used only with URI query} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{kw} & searched keyword(s); \newline \textit{used only with keyword query} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{format} & format of the result & \code{html}, \code{trig} & \code{html} \\
	\hline
	\code{aggr} & default aggregation method & \vartext{string} & \code{ALL} \\
	\hline
	\code{es} & error strategy -- handling of values for which aggregation fails & \code{IGNORE}, \code{RETURN\_ALL} & \code{RETURN\_ALL} \\
	\hline
	\code{multivalue} & default multivalue setting & 0, 1 & 0 \\
	\hline
	\code{paggr[\vartext{property}]} & aggregation method for the given property; \newline example: \code{paggr[rdfs\%3Alabel]=ANY} & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{pmultivalue[\vartext{property}]} & multivalue setting for the given property;
		\newline example: \code{pmultivalue[rdf\%3Atype]=1} & 0, 1 & \vartext{N/A} \\
	\hline
\end{tabularx}
\caption{URI and keyword query parameters}
\end{table} 

The next table lists parameters that can be used with the named graph metadata query:

\begin{table}[h!]
\centering
\label{tbl:requestFormatNG}
\begin{tabular}{|l|l|l|l|}
	\hline
	Name & Description & Possible values & Default value \\
	\hline \hline
	\code{uri} & URI of the requested named graph & \vartext{string} & \vartext{N/A} \\
	\hline
	\code{format} & format of the result & \code{html}, \code{trig} & \code{html} \\
	\hline
\end{tabular}
\caption{Named graph metadata query parameters}
\end{table}

For all queries, parameters and values are case-sensitive. Property names may be either full URIs, or prefixed names (e.g. \code{rdfs:label}). Available prefixes are managed in the administration frontend (see section \ref{sec:frontendPrefixMgmt}).

For more information about aggregation settings, see the corresponding section of Conflict Resolution specification\todo{reference}.

\subsubsection*{General aggregation methods}

\begin{description}
	\item[ALL]
		returns all conflicting values
	\item[BEST]
		value with the highest aggregated quality; in case of equality, the newest timestamp is preferred
	\item[LATEST]
		value with the newest timestamp; in case of equality, the highest aggregate quality is preferred
	\item[ANY]
		returns a single arbitrary value
	\item[CONCAT]
		concatenation of conflicting values separated by \quot{\code{;\ }}
	\item[NONE]
		retruns all conflicting values including duplicities
\end{description}

\subsubsection*{Numeric aggregation methods}

\begin{description}
	\item[MIN]
		minimum of conflicting values
	\item[MAX]
		maximum of conflicting values
	\item[AVG]
		average of conflicting values
	\item[MEDIAN]
		median of conflicting values
\end{description}

\subsubsection*{Date aggregation methods}

\begin{description}
	\item[MIN]
		the earliest date
	\item[MAX]
		the latest date
\end{description}

\subsubsection*{String aggregation methods}

\begin{description}
	\item[SHORTEST]
		the shortest string
	\item[LONGEST]
		the longest string
\end{description}

\subsubsection*{Error strategy}
The error strategy determines how to handle values that cannot be aggregated by the given aggregation method, e.g. when applying MEDIAN aggregation to a mix of numeric and date values.

Note that for some aggregations, an untyped literal may be converted to a numeric literal (\code{xsd:double}) if possible.

\subsubsection*{Multivalie parameter}
The multivalue parameter determines whether differences with other conflicting values decrease quality (\code{multivalue=0}), or not (\code{multivalue=1}). Setting multivalue to false (0) is appropriate for properties with a single value (e.g. \code{dbprop:population}), setting it to true (1) is appropriate for propertiees with multiple possible values (e.g. \code{rdf:type}).

\todo{content-negotiation}

\subsection{Query Format}

\subsubsection{URI Query \& Named Graph Metadata Query}
The value of the \code{uri} parameter must be either a full valid URI, or a prefixed name (e.g. \code{dbpedia:Berlin}).  Available prefixes are managed in the administration frontend (see section \ref{sec:frontendPrefixMgmt}).

\subsubsection{Keyword Query}
The \code{kw} parameter can contain one or more keywords separated by whitespace. If a keyword itself contains spaces, it may be enclosed in double quotes. Query Execution looks for literals that contain all of the keywords. Keywords can also contain the \code{*} wildcard, but they must begin with at least four non-wildcard characters if a wildcard is to be used.

Query Execution also looks for an exact match of the entire \code{kw} value (i.e. without any division to keywords). If the \code{kw} value is a number, then numeric typed literals will match; if the \code{kw} value is formatted as \code{xsd:dateTime}\footnote{\url{http://www.w3.org/TR/xmlschema-2/\#dateTime-lexical-representation}}, then \code{xsd:dateTime} typed literals will match.

\subsection{Results Format for URI \& Keyword Queries}
The result contains triples returned in response to the query, including relevant labels of URI resources in the result, and metadata for the triples.

\subsubsection{HTML}

The result in HTML format contains results in a human-readable form. It contains

\begin{itemize}
	\item a table with all triples in the result together with their aggregated quality and named graphs from which the triple was selected or calculated,
  \item  a table with metadata of named graphs occuring in the first table.
\end{itemize}

\subsubsection{TriG}

The result contains triples (quads) serialized in the TriG\footnote{\url{http://www4.wiwiss.fu-berlin.de/bizer/trig/}} format. The result includes:

\begin{itemize}
	\item triples returned in response to the query, each one placed in a unique named graph
  \item aggregated quality (\code{odcs:quality}) and source named graphs (\code{w3p:source}) of the above triples; subjects of these statements are the unique named graphs where the respective triples are placed
  \item  metadata of source named graphs; they may include where the data were extracted from (\code{w3p:source}), Quality Assesment score of the named graph (\code{odcs:score}) and its publisher (\code{odcs:publisherScore}), the publisher of the data (\code{w3p:publishedBy}), timestamp (\code{w3p:insertedAt}), licence (\code{dc:licence})
  \item  metadata about the query response itself -- a title (\code{dc:title}), date (\code{dc:date}), number of result triples (\code{odcs:totalResults}), the query (\code{odcs:query}) and link to each result item (\code{odcs:result})
\end{itemize}

\pagebreak

An example:

\begin{lstlisting}[caption={Example of query response in TriG}]
@prefix :        <#> .
@prefix odcs:    <http://opendata.cz/infrastructure/odcleanstore/> .
@prefix w3p:     <http://purl.org/provenance#> .
@prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .
@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc:      <http://purl.org/dc/terms/> .

<http://opendata.cz/infrastructure/odcleanstore/query/metadata/> {
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:insertedAt "2012-04-01 12:34:56.0"^^<http://www.w3.org/2001/XMLSchema#dateTime> ;
    w3p:source <http://dbpedia.org/page/Berlin> ;
    w3p:publishedBy <http://dbpedia.org/> ;
    odcs:provenanceMetadataGraph
      <http://opendata.cz/infrastructure/odcleanstore/provenanceMetadata/e0cdc9d7-e2d8-4bde>;
        
    odcs:score 0.72 ;
    odcs:violatedQARule <http://opendata.cz/infrastructure/odcleanstore/QARule/10> ;
    odcs:violatedQARule <http://opendata.cz/infrastructure/odcleanstore/QARule/20> .  

  <http://opendata.cz/infrastructure/odcleanstore/QARule/10>
    a odcs:QARule ;
    odcs:coefficient 0.8 ;
    dc:description "Procedure type ambiguous" .
        
  <http://opendata.cz/infrastructure/odcleanstore/QARule/20>
    a odcs:QARule ;
    odcs:coefficient 0.9 ;
    dc:description "Procurement contact person missing" .
        
  <http://localhost:8087/namedGraph?uri=http%3A%2F%2Fopendata.cz
      %2Finfrastructure%2Fodcleanstore%2Fdata%2Fe0cdc9d7-e2d8-4bde>
    a odcs:QueryResponse ;
    dc:title "Metadata for named graph:
      http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde" ;
    dc:date "2012-08-01T10:20:30+01:00" ;
    odcs:query "http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde";
}
    
<http://opendata.cz/infrastructure/odcleanstore/provenanceMetadata/e0cdc9d7-e2d8-4bde> {
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:provenanceMetadataProperty1 "provenanceMetadataValue1".
  <http://opendata.cz/infrastructure/odcleanstore/data/e0cdc9d7-e2d8-4bde>
    w3p:provenanceMetadataProperty2 "provenanceMetadataValue2".
}
\end{lstlisting}

\todo{JSON}
\todo{Error handling!}
\todo{Examples}


\chapwithtoc{References}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\chapter{Glossary}

\section*{RDF related}
\begin{glossarylist}
	\item[RDF] Resource Description Framework, a language for representing information about resources in the World Wide Web\footnote{\url{http://www.w3.org/RDF/}}
	\item[RDF triple] Statement about a resource expressed in the form of subject-predicate-object expression
	\item[URI] Uniform Resource Identifier, identifies RDF resources
	\item[Named graph] A set of related RDF triples (RDF graph) named with a URI\footnote{\url{http://www.w3.org/2004/03/trix/}}
	\item[RDF quad] An RDF triple plus named graph URI (subject, predicate, object, named graph)
	\item[Ontology] Representation of the meaning of terms in a vocabulary and of their interrelationships
	\item[OWL] The Web Ontology Language\footnote{\url{http://www.w3.org/TR/owl-features/}}
	\item[SPARQL] A RDF query language\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/}}
\end{glossarylist}

\section*{Data \& Data Quality}
\begin{glossarylist}
	\item[Dirty (staging) database] Database where incoming data are stored until they are processed by a processing pipeline (e.g. clean, linked to other data, etc.)
	\item[Clean database] Database where incoming data are stored after they are successfully processed by the respective processing pipeline; this database can be accessed using the Output Webservice
	\item[Named graph score (\code{odcs:score})] Quality of a single named graph estimated by the Quality Assesment component and stored in the database, expressed as a number from interval [0,1]
	\item[Publisher score] Average score of named graphs from a publisher
	\item[Aggregate quality] Quality of a triple in the results calculated by the Conflict Resolution component during query time, expressed as a number from interval [0,1]
\end{glossarylist}

\section*{User Roles}
\begin{glossarylist}
	\item[ADM] Administrator
	\item[ONC] Ontology creator
	\item[PIC] Pipeline creator
	\item[SCR] Data producer (scraper)
	\item[USR] Data consumer
\end{glossarylist}

\chapter{List of Used XML Namespaces}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
	\hline
	\ttfamily
	\textrm{\textbf{Prefix}} & \textrm{\textbf{URI}} \\
	\hline\hline
	odcs & http://opendata.cz/infrastructure/odcleanstore/ \\
	\hline
	w3p & http://purl.org/provenance\# \\
	\hline
	dc & http://purl.org/dc/terms/ \\
	\hline
	rdf & http://www.w3.org/1999/02/22-rdf-syntax-ns\# \\
	\hline
	rdfs & http://www.w3.org/2000/01/rdf-schema\# \\
	\hline
	owl & http://www.w3.org/2002/07/owl\# \\
	\hline
	xsd & http://www.w3.org/2001/XMLSchema\# \\
	\hline
	dbpedia & http://dbpedia.org/resource/ \\
	\hline
	dbprop & http://dbpedia.org/property/ \\
	\hline
\end{tabular}
%\caption{List of used XML namespaces}
\end{table} 

\end{document}
