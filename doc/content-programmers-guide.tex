\include{header}
\newcommand{\version}{0.3.6}
\newcommand{\documentname}{\refprogrammersguide}


\begin{document}

\include{titlepage}

\renewcommand{\contentsname}{Contents}
\tableofcontents
\bigskip

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% !List of features and intended use-cases (zduvodneni featur FE)
	% rrequirements - functional, intraface, non-functional?


% release proces
% issues redmine - debugging & spol
% used technologies
% FE
	% zdokumentovat, co je potřeba pro přidání DN template apod.
  % FE - popsat, že to je webová aplikace, dá se spouštět pod tomcatem, odkaz na install manual
  % význam abstraktních Dao
  % uncommitted tabulky
  % jak přidat novou stránku


	% třídy
		% webfrontend.behavior - Wicket Behavior implementations
		% webfrontend.bo
		% webfrontend.configuration
		% webfrontend.core
			% DaoLookupFactory, ODCSWebFrontendApplicatoin .java/properties, ODCSWebFrontendSession, URLRouter
		% webfrontend.core.components - custom Wicket components (buttons, labels, form components etc.)
		% webfrontend.core.model - Wicket models and data providers
		% webfrotend.dao
		% webfrontend.pages - pages, panels, ...
		% webfrontend.util
		% webfrontend.validators

	% naming conventions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\odcs is a~server application for management of Linked Data written in Java. It stores data in RDF, processes them and provides integrated views on the data.

This document serves as the main documentation for developers. It describes basic architecture, implementation, development process, used technologies and other important information relevant for people who want to participate in the development of \odcs.

\section{What is ODCleanStore}
\odcs accepts arbitrary RDF data and metadata through a SOAP webservice (\term{Input Webservice}). The data is processed by \term{transformers} in one of a~set of customizable \term{pipelines} and stored to a~persistent store (OpenLink Virtuoso database instance). The stored data can be accessed again either  directly through a SPARQL endpoint or through \term{Output Webservice}. Linked Data consumers can send queries and custom query policies to Output Webservice and receive (aggregated/integrated) RDF data relevant for their query, together with information about provenance and data quality. 

\section{Related documents}
More detailed information about \odcs from the perspective of a user or an administrator can be found in related documents \quot{\refusermanual} and \quot{\refadminmanual}. \refusermanual also contains definition of user roles, glossary of terms etc.

Other working documents related to development are located at the project's page at SourceForge\footnote{\url{https://sourceforge.net/p/odcleanstore/wiki/For\%20developers/}}. The Wiki tool at SourceForge is used for working documents, discussion of new features, description of testing scenarios etc. Not all pages are up-to-date, however, and this document is authoritative in case of conflicts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Requirements}
% todo
% features, interfaces, administration, future use
% motivace, ostatní existující řešení, v čem je to nové


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{ODCleanStore overview} 
An overview of how \odcs works is depicted on \figurename~\ref{fig:odcsInternal}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{images/odcs-internal.png}
    \caption{Overview of ODCleanStore architecture}
	\label{fig:odcsInternal}
\end{figure}

The diagram lists all main functional units in \odcs:

\begin{itemize}
	\item \importantterm{Engine.}
		Engine runs the whole server part. It realizes all data processing and starts Input and Output Webservices.

		\begin{itemize}
			\item \importantterm{Input Webservice}. SOAP webservice that accepts new data and queues them for processing in the dirty database.
			\item Pipeline processing. Processes queued data by running a series of transformers in a pipeline on it and moves the dat to the clean database.
			\item \importantterm{Ouptut Webservice}. REST webservice for querying over data in the clean database.
		\end{itemize}
	\item \importantterm{Query Execution \& Conflict Resolution.}
		\QE retrieves all data and metadata relevant for a query asked via Output Webservice. Conflict Resolution then resolves conflicts in the retrieved data, including resolution of \code{owl:sameAs} links.
	\item Predefined transformers.
		Transformers used for data processing that are included by default in \odcs.
		\begin{itemize}
			\item \importantterm{Quality Assessment}. Estimates quality of data based on user-defined or generated rules.
			\item \importantterm{Data Normalization}. Transformations of data based on user-defined or generated rules.
			\item \importantterm{Linker}. Generates links (e.g. \code{owl:sameAs}) between resources in the processed data and contents of the clean database.
			\item Other transformers -- Other transformers such as Quality Aggregator, Blank Node Remover etc.
		\end{itemize}
	\item \importantterm{Administration Frontend.}
		Web application written in Java from which \odcs can be managed. In \FE, the user can define pipelines for data processing, rules for transformers, manage ontologies, Output Webservice settings etc.
\end{itemize}

Each of these parts will be described later in this document. In the source code, the components are divided into several maven projects described in Section \ref{sec:mavenBuild}.

\section{Important concepts}
\odcs is about data. More specifically, it works with data represented in RDF\footnote{\url{http://www.w3.org/TR/2004/REC-rdf-primer-20040210/}}. \odcs implements three tasks regarding data:

\begin{enumerate}
	\item Data processing
	\item Storing data
	\item Querying over stored data
\end{enumerate}

\subsection{Data Processing}
\label{sec:dataProcessing}
Data processing is realized by \term{transformers} that are applied to data being processed by Engine in a \term{pipeline}. A transformer can be any class implementing the \code{Transformer} interface but typically it only manipulates (change, add, delete) processed data in database. Several transformers ship with \odcs, such as Quality Assessment, Linker or Data Normalization.

It is important to distinct between a \term{transformer} and \term{transformer instance}. By \term{transformer}, we mean the Java class which implements the \code{Transformer} interface and is registered in ODCleanStore administration (managed by users in role Administrator). \term{Transformer instance} is assignment of such transformer to a pipeline. For example, the Quality Assessment transformer is registered in ODCleanStore by default. The user can create two pipelines and assign the Quality Assessment transformer to each of them, thus creating two transformer instances.

Some transformers can be configured in \FE by \term{rules}. In general, these rules are grouped to \term{rule groups}. Rule groups can be then assigned to transformer instances.

See also Section \ref{sec:dataLifecycle} \nameref{sec:dataLifecycle}.

\subsection{Storing Data}
Data are stored using Open Link Virtuoso RDF database. Two instances of this database are used for every deployment of \odcs:

\begin{itemize}
	\item \term{Dirty (staging) database} -- contains data that are currently being processed. Contents of this instance is not directly visible for data consumers (users in role USR).
	\item \term{Clean database} -- contains already processed data that are accessible through the Output Webservice to data consumers (users in role USR).
\end{itemize}

\subsection{Querying over Stored Data}
Querying over stored data is realized by Output Webservice which supports several types of queries (see \refusermanual). For retrieval of results and resolving conflicts, Output Webservice uses components \QE (Section \ref{sec:QE}) and Conflict Resolution (Section \ref{sec:CR}).

\subsection*{}

See also Glossary in Appendix \ref{chap:glossary} for explanation of important concepts.

\section{Data Lifecycle}
\label{sec:dataLifecycle}

The lifecycle of data inside \odcs is as follows:

\begin{enumerate}
  \item RDF data (and additional metadata) are accepted by Input Webservice and stored as a~named graph to the {dirty database}. Data can be uploaded by any third-party application registered in \odcs.
  \item Engine successively processes named graphs in the dirty database by applying a~pipeline of transformers to it; the applied pipeline is selected according to the input metadata.
  \item Each transformer in the pipeline may modify the named graph or attach new related named graphs (such as a named graph with mappings to other resources or results of quality assessment).
  \item When the pipeline finishes, the augmented RDF data are populated to the {clean database} together with any auxiliary data and metadata created during the pipeline execution.
  \item Data consumers can use Output Webservice to query data in the clean database. Output Webservice provides several basic types of queries -- URI query, keyword and named graph query; in addition, metadata about a~given named graph can be requested. The response to a~query consists of relevant RDF triples together with their provenance information and quality estimate. The query can be further customized by user-defined conflict resolution policies.\\
	Data in the clean database can be also queried using the SPARQL query language. While SPARQL queries are more expressive, there is no direct support for provenance tracking and quality estimation. 
  \item When transformer rules change, the administrator may choose to re-run a~pipeline on data already stored in the clean database. Copy of this data is created in the dirty database where it is processed by the pipeline. After that, the processed version of data replaces the original in the clean database.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\section{Architecture}

\subsection{Architecture Evolution}
The architecture that is depicted on \figurename~\ref{fig:odcsInternal} was chosen for several reasons.

First, the division into components is natural with regard to the original specification of the software project. All important components are included and we have also kept other concepts, such as two dataspaces for clean and dirty data, or the flexibility of data-processing pipeline.

Second, the selected architecture allowed a clear division of work and enabled a relative independence of each component.

Third, it is a result of a long process of analysis. From the initial vision and  requirements specification, which suggested an abstract concept of tranformers run in arbitrary order on data, we extracted several most important cases of transformers that were tightly bound to the system and could run only in a fixed order, only to generalize them back to transformers with a~very simple interface and flexible pipelines.


\subsection{Architectural Features}

\subsubsection{Components}
\odcs consists of several components that are (mostly) loosely-coupled only through a simple interface specified in advance. This made it easy divide tasks among developers and enabled them to work independently.

\subsubsection{Internal Interfaces}
In order to minimize interfaces between parts of \odcs, to minimize system requirements and to make the system more robust, we decided to prefer communication through data shared in database.

There is no direct interface between Engine and \FE, but the \FE saves all configuration to a relational database from which the Engine can retrieve it. This enbles updates of settings in transactions, prevents synchronization issues and enables the two parts to run completely independendently (even on separate machines).

Transformers run in a pipeline are isolated and don't know about each other. Instead of passing data to be transformed through an interface in memory, only the names of named graphs where data are stored are passed to each transformer. This enables to write transformers that are oblivious to \odcs as much as possible and only need to work with data by manipulating the database. Also, it gives transformers the full power of the SPARQL/SPARUL language. Although we should note that in practice, the transformer implementation may be tied with the use of Virtuoso as the underlying database, this is not such a downside because Virtuoso is one of the most popular RDF databases.

\subsubsection{Extensibility}
The main point of extensibility are custom transformers (see Section \ref{sec:customTransformers}). Because one only needs to implement a simple interface and is not bound to any specific technology (except of the limitations of the underlying Virtuoso database), transformers provide a very powerful way how to extend data processing capabilities.

\subsubsection{Interoperability}
The external interfaces are implemented using standard technologies (SOAP for Input Webservice, REST for Output Webservice) and standard formats (RDF/XML, Turtle/Notation3). This should minimize the effort for integration with third-party applications communicating with \odcs. We also provide a Java library for accessing the Input Webservice to futher minimize the effort.

\subsubsection{Used Technologies}
The choice of Java for implementation and Virtuoso as the underlying database ensure platform independence.

Since Java is a very wide-spread language, \odcs can be extended with minimum effort for learning new technologies (e.g. when adding new transformers).

\subsubsection{Scalability}
 Although currently the Engine is processing data sequentially, it is designed for parallel processing in the future. It could even be extended to work in a distributed manner on several machines.

 Most of the work the Input Webservice has is with data processing in pipelines. Since pipelines can run independently, each Engine instance could even use a dedicated database instance for dirty data.

 On the other hand, the Output Webservices uses the clean database in a read-only manner and thus could be also deployed on several database instances if database replication is put in place.

\section{Other requirements}
The assignment of the project impose several additional requirements. This section lists how they were satisfied.

\reqparagraph{It should be easy to incorporate other components, such as a component computing popularity of the data sources.} This requirement is satisfied with the introduction of custom transformers.

\reqparagraph{The application will involve graphical user interface enabling management of all kinds of policies etc.} \FE enables management of all relevant settings. In addition, several user roles are supported and user accounts can be managed from \FE.

\reqparagraph{The application will run at least on Windows 7, Windows Server 2008, Linux.} \odcs requires Java Runtime Environment and Virtuoso installation, both supported on all of the listed platforms. \odcs has been tested on Windows 7, Windows Server 2008 and Debian distribution of Linux.

\reqparagraph{Application will be freely available under Apache Software License.} \odcs is published under the required license (see \refadminmanual) and source codes available at a public repository at SourceForge.net

\section{Used technologies}

\subsection{Implementation Language}
The chosen language for implementation is Java. The reason is that there are many libraries and tools required for implemenation accessible in Java, it enables platform independence (one of the requirements on \odcs) and also it is very wide-spread so that developers don't need to learn a new syntax.

\subsection{Database}
Openlink Virtuoso\footnote{\url{http://virtuoso.openlinksw.com/}} is used as the underlying data store. It is the most popular RDF storage with a solid support. Both a commercial version and Open Source edition\footnote{\url{http://virtuoso.openlinksw.com/dataspace/dav/wiki/Main/}} exists.

RDF data store provided by Virtuoso supports reasoning, most notably \code{owl:sameAs} link resolution, which proved essential for Query Execution/Conflict Resolution components. Virtuoso also provides a relational database which relieves us from the need of another database for that purpose.

The downside is somewhat buggy behavior (especially SPARQL query parsing) and lack of working support for transactions with RDF data.

\subsection{Administration Frontend}
Apache Wicket\footnote{\url{http://wicket.apache.org/}} is used for implemenation of the \FE. It is a component system for writing web applications in Java. Advantages are proper mark-up and logic separation, POJO data model and rapid development of this particular type of web application. Wicket was also chosen by our sister project Strigil\footnote{\url{http://strigil.sourceforge.net/}} so a more tight integration of the two tools may be possible in the future.

Spring\footnote{\url{http://www.springsource.org/}} is used for simplified access to the relational database and transaction management. Use of Hibernate\footnote{\url{http://www.hibernate.org/}} for the DAO layer was rejected because of problems with integration with Wicket.

\subsection{Libraries}
\subsubsection*{Jena}
Apache Jena\footnote{\url{http://jena.sourceforge.net/}} is a library for manipulation with RDF data. It supports represenation of RDF data in memory, parsing, loading and updating of RDF models. In \odcs, it is used mainly for representation of RDF triples (quads) and serialization of RDF, because other features proved problematic when used with large data.

We chose Jena over its alternative Sesame\footnote{\url{http://www.openrdf.org/}} because it supports working with named graphs through NG4J and we had previous experience with it.

\subsubsection*{NG4J}
NG4J\footnote{\url{http://wifo5-03.informatik.uni-mannheim.de/bizer/ng4j/}} extends Jena with named graphs API.

\subsubsection*{Restlet}
Restlet\footnote{\url{http://www.restlet.org}} is an open source lightweight RESTful web framework for Java. Output Webservice is built on Restlet.

\subsubsection*{SLF4J}
SLF4J\footnote{\url{http://www.slf4j.org/}} is a flexible and efficient library used for logging. 

\subsection{Development tools}
\subsubsection*{Maven}
Apache Maven\footnote{\url{http://maven.apache.org/}} is used as the build tool. It was chosen over Apache Ant because Maven saves work with its many available plugins (such as Maven Jetty Plugin) and offers simple management of dependencies.

\subsubsection*{Git}
Git is used as the version control system. We used it because it enables simple manipulation with branches and merging but most importantly it is less dependent on a central server whose potential unavailability was identified as a potential risk.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Setting up Development Environment}

\section{Quick Start}
\subsection{Tools}
	In order to to prepare environment for building \odcs, first make sure to have installed all necessary tools:

\begin{itemize}
	\item Java Development Kit\footnote{\url{www.oracle.com/technetwork/java/javase/downloads/}} version 6 or newer
	\item Git version control system\footnote{\url{http://git-scm.com/}}
	\item Apache Maven\footnote{\url{http://maven.apache.org/}}
\end{itemize}

Also, make sure you have all the binaries used in the following steps on your classpath. 

\subsection{Obtaining sources} 
\odcs sources are hosted at SourceForge.\footnote{\url{http://sourceforge.net/p/odcleanstore/code/}} Use git to check out the sources:

\begin{verbatim}
      git clone git://git.code.sf.net/p/odcleanstore/code odcleanstore-code
\end{verbatim}

The above command will create a local clone of the repository in \code{odcleanstore-code} directory.

\subsection{Building binaries}
Move to directory \code{odcleanstore} within your local clone of the repository which contains the root maven project (\code{pom.xml}). Then build the project using maven:

\begin{verbatim}
      cd odcleanstore-code/odcleanstore
      mvn clean package install
\end{verbatim}

After that, Engine binaries can be found in directory \code{odcleanstore/engine/target} and the WAR file of \FE at \code{odcleanstore/webfrontend/target}. Now you can deploy the application as described in \refadminmanual. \todo{jestli tam bude jenom popis instalatoru, mel by se rucni deploy popsat tady a pridat referenci na instalaci Virtuosa}

\section{Repository structure}
\subsection{Branches}
There are several branches in the git repository. The latest development version of is on branch \code{master}. Then there are release branches for each release named \code{release-0.1.x}, \code{release-0.2.x} etc. which contain stable versions for  releases  packages. Each commit that where used to prepeare a release package is labeled with a tag \code{release}-\varcode{version}. Finally, there are feature branches prefixed with \code{feature-}.

Common development takes place on branch \code{master}. New feature branches are created for features that may or may not be accepted or that need time to be finished before they are applied to \code{master}. When whey are finished, they are merged to \code{master} and the branch may be removed in time. Release branches stem from \code{master} for every new major release. Fixes and modifications for minor releases take place on release branches and may be merged from/to \code{master}.

\subsection{Directory structure}
This is an outline of directory structure in the git repository:

\begin{dirlist}
	\item[data/] \mbox{}
		\begin{dirlist}
			\item[initial\_db\_import/] -- database import files
				\begin{dirlist}
					\item[clean\_db/] -- SQL files to be imported to the clean database
					\item[dirty\_db/] -- SQL files to be imported to the dirty database
				\end{dirlist}
			\item[odcs\_configuration/] -- the default \odcs configuration file
			\item[virtuoso\_configuration/] -- configuration files for Virtuoso database instances
		\end{dirlist}
	\item[doc/] -- documentation sources (in \LaTeX)
	\item[odcleanstore/] -- Java sources
		\begin{dirlist}
			\item[backend/] -- sources of \code{odcs-backend} artifact (transformers, Query Execution, Conflict Resolution)
			\item[comlib/] -- sources of \code{odcs-comlib} artifact (code related to sending data to Input Webservice)
			\item[conf/] -- configuration files for development in Eclipse
			\item[core/] -- sources of \code{odcs-core} artifact (common code shared by other artifacts)
			\item[engine/]  -- sources of \code{odcs-engine} artifact (Engine component)
			\item[inputclient/]  -- sources of \code{odcs-imputclient} artifact (Java client for Input Webservice)
			\item[simplescraper/]  -- simple Input Webservice import tool
			\item[simpletransformer/]  -- example of a custom transformer
			\item[webfrontend/]  -- sources of \FE
			\item[pom.xml] -- the root maven POM file
		\end{dirlist}
\end{dirlist}


\section{Maven build}
\label{sec:mavenBuild}
The project is divided into several artifacts. The root POM (\code{pom.xml}) in \code{odcleanstore} directory defines the parent project while each component of \odcs is in a separate artifact in subdirectories of \code{odcleanstore}. These artifacts are:

\begin{itemize}
	\item \code{odcs-core} -- common code shared by other artifacts; also defines interfaces for custom transformers
	\item \code{odcs-backend} -- predefined transformers, Query Execution and Conflict Resolution
	\item \code{odcs-engine} -- Engine component (running Input and Output Webservice and the data processing queue)
	\item \code{odcs-comlib} -- components shared by \code{odcs-inputclient} and Input Webservice
	\item \code{odcs-webfrontend} -- \FE web application
	\item \code{odcs-inputclient} -- client library for Input Webservice (provides a Java API for accessing the webservice)
	\item \code{odcs-simplescraper} -- a simple command-line tool for importing data through Input Webservices
	\item \code{odcs-simpletransformer} -- example of a custom transformer
\end{itemize}


It is recommended to always build using the root POM. Building from subprojects may require issuing \code{mvn install} on the root POM first. The entire project can be build with the following command:

\begin{verbatim}
      cd odcleanstore-code/odcleanstore
      mvn clean package
\end{verbatim}

There are two profiles in addition to the default one in the root POM.

\begin{itemize}
	\item \code{javadoc} profile -- enables generation of javadoc (which is disabled in the default profile). 
	\item \code{systest} profile -- enables unit tests in \code{systest/} subdirectories, which test functionality related to the database. In order to run these tests, a new Virtuoso instance with settings as in \code{/data/virtuoso\_configuration/virtuoso.ini-test} must be set up first. \todo{ref to Virtuoso installation guide}
\end{itemize}

Maven build for the selected profile can be executed with command line option \code{-P}:

\begin{verbatim}
      mvn clean package -P javadoc
      mvn clean package -P systest	
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Shared Code}
Code shared by multiple components in \odcs is extracted to Maven artifact \code{odcs-core}. It contains:

\begin{itemize}
	\item Classes for accessing configuration from the global configuration class.

		Java package: \code{cz.cuni.mff.odcleanstore.configuration}
	\item Helper classes for accessing the Virtuoso database.

		Java package: \code{cz.cuni.mff.odcleanstore.connection}

	\item Helper classes for imports of data to Virtuoso database from files.

		Java package: \code{cz.cuni.mff.odcleanstore.data}

	\item Classes related to transformer interface.

		Java package: \code{cz.cuni.mff.odcleanstore.transformer}

	\item Definitions of vocabularies used in \odcs.

		Java package: \code{cz.cuni.mff.odcleanstore.vocabulary}

	\item And other utility and helper classes, such as unique URI generators, filesystem utilities etc.
\end{itemize}

\section{Configuration}
The global configuration can be accessed using static methods of \code{ConfigLoader}. First, the configuration must be loaded using \code{loadConfig()} methods. Both Engine and \FE ensure that this is done as soon as they start so that other components may access the configuration already when they are loaded. See \refadminmanual for description of the configuration file.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dia-core-config.png}
    \caption{Diagram of (selected) configuration classes}
	\label{fig:configurationClasses}
\end{figure}

Configuration for each component is in classes called \code{XXXConfig} inheriting from \code{ConfigGroup}. Each instance of \code{ConfigGroup} loads configuration relevant only for that component so as to minimize dependencies. Configurations for all components are grouped in \code{Config} class accessible from \code{ConfigLoader}.

\section{Database Access}
Classes \code{JDBCConnectionCredentials} and \code{SparqlEndpointConnectionCredentials} are containers for information necessary for connecting to the database.

\FE uses its own database access layer using Spring templates. The rest of \odcs should use classes \code{VirtuosoConnectionWrapper} and \code{WrappedResultSet}. These two classes provide methods for both querying and updating the relational database and working with SPARQL. They also provide methods for conversion of Virtuoso SQL types to Java types, working with transactions etc. Note that SPARQL update operations should be executed with log level set to AUTOCOMMIT (default).

Use of \code{VirtuosoConnectionWrapper} is also recommended for implementation of custom transformers. An example of how it can be used is in  \lstlistingname~\ref{lst:virtuosoConnectionWrapper}.

\begin{lstlisting}[caption={Example of programmatic access to Virtuoso database from a transformer},label=lst:virtuosoConnectionWrapper]

  @Override
  public void transformGraph(TransformedGraph inputGraph, TransformationContext context)
      throws TransformerException {
    VirtuosoConnectionWrapper connection = null;
    WrappedResultSet resultSet = null;
    try {
      connection = VirtuosoConnectionWrapper.createConnection(
          context.getDirtyDatabaseCredentials());

      String query1 = "SPARQL SELECT ?s WHERE {?s ?p ?o}";
      resultSet = connection.executeSelect(query1)
      while (resultSet.next()) {
        String s = resultSet.getString("s");
      }

      String query2 = "SPARQL INSERT INTO <a> { <b> <c> <d> }";
      connection.execute(query2);
    } catch (DatabaseException e) {
      throw new TransformerException(e);
    } catch (SQLException e) {
      throw new TransformerException(e);
    } finally {
      if (resultSet != null) { resultSet.closeQuietly(); }
      if (connection != null) { connection.closeQuietly(); }
    }
  }
\end{lstlisting}

\section{Data Import}
When importing data to Virtuoso, one should use the \code{GraphLoader}  class or methods in \code{VirtuosoConnectionWrapper}. These make Virtuoso import the data itself. Other methods, such as using the Jena library may fail when used with too large data. Note that when importing from a file, it must be in directory listed in \code{DirsAllowed} directive of Virtuoso. See also \code{engine.clean\_import\_export\_dir} configuration option.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Engine}

\todo{jak se to spousti, vypina, celkova architektura}

\section{Pipeline Processing}

\section{Input Webservice}

\section{Output Webservice}
\subsection{Purpose}
Output Webservice is a RESTful webservices for queries over data in the clean database. More details about types of queries and request format are described in \refusermanual.

\subsection{Implementation}
Output Webservice is built on top of the Restlet library. \code{OutputWSService} started by Engine registers a Restlet application implemented by class \code{Root} which sets up URI routes and handlers for each type of query.

Each type of query is handled by a class inheriting from \code{QueryExecutorResourceBase} which in turn implements Restlet \code{ServerResource}. This base class loads necessary configuration and handles requests (methods annotated with Restlet \code{@Get} and \code{@Post} annotations) -- parses request parameters (as described in \refusermanual), delegates the execution to the abstract \code{execute()} method implemented in child classes and handles returning of a proper response in case of an error.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{images/dia-outputws-resources.png}
    \caption{Diagram of selected Output Webservice classes}
	\label{fig:outputwsResources}
\end{figure}


Classes implementing the actual execution of the query are \code{UriQueryExecutorResource}, \code{KeywordQueryExecutorResource}, \code{NamedGraphQueryExecutorResource} and \linebreak[4] \code{MetadataQueryExecutorResource}. They redefine the abstract \code{execute()} method where query-specific parameters are parsed and the execution is delegated to the Query Execution component. Instance of the \code{QueryExecution} class is shared between requests in order to utilize caching implemented in Query Execution. Finally, the result of the query is formatted and sent to the user.

\subsection{Output Formatters}
Query results returned from Query Execution are formatted using the format requested by the user. Formatting is done by classes implementing \code{QueryResultFormatter}.

The default formatter is \code{HTMLFormatter} which outputs results in HTML. \code{RDFXMLFormatter} and \code{TriGFormatter} inherit from \code{RDFFormatter} and output results in RDF/XML and TriG, respectively. \code{DebugFormatter} formats result for output to console and is not accessible for users.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dia-outputws-formatters.png}
    \caption{Diagram of output formatters hierarchy}
	\label{fig:outputwsFormatters}
\end{figure}

\subsection{Extending}
In order to add a new type of query to Output Webservice, the following steps should be taken:

\begin{enumerate}
	\item Implement a new \code{ServerResource} executing the query, preferably inheriting from \linebreak[4] \code{QueryExecutorResourceBase}. Typically, the actual query will be delegated to the Query Execution component -- see Section \ref{sec:qeExtending}.
	\item Register the new \code{ServerResource} in method \code{Root\#createInboundRoot()}.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Query Execution}
\label{sec:QE}

\section{Purpose}
The purpose of \QE is to retrieve result for a query (asked through Input Webservice), resolve conflicts using the Conflict Resolution component and return result.

Triples that \QE retrieves are:
\begin{enumerate}
	\item \label{item:queryTriples} Triples relevant for the query (e.g. containing the given URI).
	\item Triples with metadata about named graphs containing triples from (\ref{item:queryTriples}).
	\item Triples containing human-readable labels for URI resources occuring in triples from (\ref{item:queryTriples}).
\end{enumerate}
A special case is the metadata query which retrieves only named graph metadata.

Because the result of conflict resolution depends on the data it is given, \QE and \CR are not independent but rather \QE extracts exactly the data that \CR needs and calls it directly.

\section{Interface}
The public interface of the \QE component is represented by class \code{QueryExecution}.

 This class exposes methods for executing all kinds of supported queries and returns result as an instance of \code{MetadataQueryResult} (wraps collection of provenance metadata triples and other metadata) or \code{BasicQueryResult} (wraps collection of \code{CRQuad}s returned from \CR plus metadata). The query can be further parametrized by passing \code{QueryConstraintSpec} and \code{AggregationSpec} affecting the retrieved data and the conflict resolution process, respectively.

 \code{QueryExecution} is thread-safe and its instance should be kept between requests in order to effectively utilize caching.

\section{Implementation}
The actual implementation is in classes inheriting from \code{QueryExecutorBase}, each implementing one type of query: \code{URIQueryExecutor}, \code{KeywordQueryExecutor}, \code{NamedGraphQueryExecutor} and \code{MetadataQueryExecutor}. These classes are called internally from the \code{QueryExecution} class. Implementing classes are in Java package \code{cz.cuni.mff.odcleanstore.queryexecution}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dia-qe.png}
    \caption{Diagram of main Query Execution classes}
	\label{fig:qeClasses}
\end{figure}

For each query, the following steps are executed: input is validated, result quads retrieved from the database, metadata and labels are retrieved from the database and conflict resolution is applied to the result.

To improve performance, values that are used for each query but rarely changed are cached. Cached values are: default aggregation settings, prefix mappings and label properties.

\section{Extending}
\label{sec:qeExtending}
In order to implement a new type of query, the following steps should be taken:

\begin{enumerate}
	\item Create a class implementating the new query, preferably inheriting from \code{QueryExecutorBase}.
	\item Extend \code{QueryExecution} class with method for executing the query.
	\item Extend Input Webservice to provide access to the new query for users.
\end{enumerate}

\section{Database}
\QE retrieves RDF data from the clean database instance. Because Virtuoso doesn't fully support transactions with RDF data, the clean database may contain incomplete data partially inserted by Engine. In order to filter such data from the result, \QE ignores all named graphs whose URI starts with an agreed prefix\footnote{\code{http://opendata.cz/infrastructure/odcleanstore/internal/hiddenGraph/}} given by Engine to such named graphs.

In addition, \QE loads settings from the following tables in relational database (see Appendix \ref{chap:reldb}):

\begin{itemize}
	\item \dbodcs{qe\_label\_properties}
	\item \dbodcs{cr\_properties} 
	\item \dbodcs{cr\_settings} 
	\item and tables referenced from tables above
\end{itemize}

\todo{zmínka o engine prefixu}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conflict Resolution}
\label{sec:CR}

\section{Purpose}
\section{Interface}
\section{Implementation}
\section{Extending}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Transformers -- Introduction}
In this section, by a transformer we mean a Java class implementing the \code{Transformer} interface shown in \lstlistingname~\ref{lst:transformer} (and other related classes used for implementation).

\begin{lstlisting}[caption={Transformer interface},label=lst:transformer]
  package cz.cuni.mff.odcleanstore.transformer;
  public interface Transformer {
    void transformGraph(TransformedGraph inputGraph, TransformationContext context)
         throws TransformerException;

    void shutdown() throws TransformerException;
}
\end{lstlisting}

The purpose of a transformer is to somehow process data. The data are not passed in memory, but rather stored in the (dirty) database instance and only the URI of the named graph to be processed and connection credentials for accessing the database are given to the transformer. This should minimize the need of complicated interfaces for data passing, make it easier to work with large data, let the transformer choose its own method of accessing the database and give it the full power of SPARQL/SPARUL (as implemented in Virtuoso).

The actual data processing should be implemented in the \code{transformGraph()} method. All required information is passed in its arguments, one with information about the processed graph and one about the environment -- see \lstlistingname{}s~\ref{lst:transformedGraph} and \ref{lst:transformationContext}.

The \code{shutdown} method is called when Engine shuts down and can be used e.g. to release acquired resources.

\begin{lstlisting}[caption={TransformedGraph interface},label=lst:transformedGraph]
  package cz.cuni.mff.odcleanstore.transformer;
  import java.util.Collection;

  public interface TransformedGraph {
    String getGraphName();
    String getGraphId();
    String getMetadataGraphName();
    String getProvenanceMetadataGraphName();
    Collection<String> getAttachedGraphNames();
    void addAttachedGraph(String attachedGraphName) throws TransformedGraphException;
    void deleteGraph() throws TransformedGraphException;
    boolean isDeleted();
  }
\end{lstlisting}

\begin{lstlisting}[caption={TransformationContext interface},label=lst:transformationContext]
  package cz.cuni.mff.odcleanstore.transformer;
  import java.io.File;
  import cz.cuni.mff.odcleanstore.connection.JDBCConnectionCredentials;

  public interface TransformationContext {
    JDBCConnectionCredentials getDirtyDatabaseCredentials();
    JDBCConnectionCredentials getCleanDatabaseCredentials();
    String getTransformerConfiguration();
    File getTransformerDirectory();
    EnumTransformationType getTransformationType(); /* NEW or EXISTING */
  }
\end{lstlisting}

\section{Transformer Instance Configuration}
Each instance of a transformer in a pipeline may have its own configuration (for explanation of the difference between \term{transformer} and \term{transformer instance}, see Section \ref{sec:dataProcessing}). From the point of view of a transformer, it is a plain string which can be obtained by calling the \code{getTransformerConfiguration()} method.

This configuration string can be edited in \FE. The transformer may use the value in any way it needs, e.g. it may contain XML configuration, the recommended practice is to use the Java \code{Properties} file format, however. This format is used by transformers included by default in \odcs unless stated otherwise.

Instances of important transformers (\QA, \DN, \OI) can be also configured by assigning rule groups to them in \FE.

\section{Contract between Engine and Transformers}
\label{sec:transformerContract}
Although Virtuoso doesn't fully support transactions over RDF data, data processing in \odcs is implemented so as to keep data consistent. In order to make it work, however, a contract between Engine and transformers must be satisfied.

The Engine ensures that:

\begin{itemize}
	\item When a transformer is applied to a transformed graph, no other transformer (in the same nor different pipeline) is applied to it. In other words, the transformed graph is not changed externally while a transformer is working on it.
	\item If the transformer throws an exception, all changes made in the pipeline on the graph in the dirty database are safely reverted (and the state of the graph is changed to WRONG). The graph may be processed again later. If the transformer was run on a graph already in the clean database, the version in the clean database in intact.
	\item Transformers may use the directory given by the \code{getTransformerDirectory()} method for they own purposes, e.g. storing temporary files, log files etc. It is a subdirectory named as the ID of the executed transformer instance inside the \quot{working directory} of the transformer (configurable in \FE).

		Data specific for one transformer instance may be stored in this directory. Data shared by all instances of the same transformer may be stored in the parent directory of that returned by \code{getTransformerDirectory()}. Engine ensures that this parent directory will be the same for all instances of the same transformer (unless working directory is changed in \FE, of course).
\end{itemize}

In return, transformers should satisfy:

\begin{itemize}
  \item Transformers may add/update/delete data in the \code{payload} graph, metadata graph, \code{provenance} graph or attached graphs. It may also add data to new graphs, but the transformer must
		\begin{enumerate}
			\item register the graph by calling \code{addAttachedGraph()} \textit{before} it writes any data to it,
			\item make sure that the name of the new graph is unique in the database (transformer may use the \code{getGraphId()} method to create names unique for each named graph).
		\end{enumerate}
		Transformers shouldn't modify contents of the dirty database in any other way.
	\item Transformers may access the clean database, but should use it only for reading. Because other transformer in the pipeline may fail, the changes executed by the current transformer in the dirty database may be discarded but changes in the clean database would be kept which may cause inconsistencies. The same applies should the transformer execute any other persistent actions.
	\item Transformer should only use the directory given by \code{getTransformerDirectory()} or its parent directory for accessing the filesystem.
\end{itemize}

\section{Custom Transformers}
\label{sec:customTransformers}
The administrator may extend data-processing capabilities of \odcs by adding new custom transformers. How to do so is described in \refadminmanual.

From the technical point of view, a transformer implementation must implement the \code{Transformer} interface. This interface and other necessary classes are included in maven artifact \code{odcs-core}, so that only this artifact need to be referenced.

Note that custom transformers should satisfy conditions listed in Section \ref{sec:transformerContract}.

\chapter{Transformers included in ODCleanStore}

\section{Quality Assessment \& Quality Aggregator}
\section{Data Normalization}
\section{Linker}
\section{Other Transformers}
\subsection{Blank Node Remover}
Blank Node Remover is a simple transformer for replacing of blank nodes in the \code{payload} graph with unique URI resources. It is implemened in class \code{ODCSBNodeToResourceTransformer}.

The generated URIs have format {\varcode{prefix}\varcode{random UUID}-\varcode{Virtuoso nodeID}}. The transformer guarantees that occurrences of the same blank node within the transformed graph
 will be assigned the same URI, however, occurrences of the blank node in other graphs will be assigned a different URI when they are processed by the transformer.

Value of \code{input\_ws.named\_graphs\_prefix} configuration option concatenated with \quot{genResource/} is used as the default value of the \varcode{prefix} part. It can be overriden by \code{uriPrefix} option in transformer instance configuration.

\subsubsection{Configuration}
Possible configuration options for an instance of this transformer:
\begin{configlist}
	\item[uriPrefix] Sets the prefix of URIs generated in place of blank nodes.
\end{configlist}

\subsection{Latest Update Marker}
Latest Update Marker is an internal transformer for marking the latest version of a named graph with \code{odcs:isLatestUpdate} property. The marker may be used when accessing the clean database directly through the SPARQL endpoint. Latest Update Marker is implemented in class \code{ODCSLatestUpdateMarkerTransformer}.

A named graph $A$ is considered an update of named graph $B$ if:
\begin{enumerate}
		\item Named graphs $A$ and $B$ have the same update tag, or both have an unspecified (\code{null}) update tag.
	  \item Named graphs $A$ and $B$ were inserted by the same (SCR) user.
	  \item Named graphs $A$ and $B$ have the same set of sources in metadata.
\end{enumerate}

The transformed graph will be labeled as the latest version by adding  the triple\linebreak \varcode{payload-graph}-\code{odcs:isLatestUpdate}-\quot{1}. If it updates another graph in the clean database, the other graph will be unmarked as being the latest version.

This transformer is automatically added by Engine to the end of every pipeline. This is neccessary because the transformer may modify the clean database and therefore should ensure that the pipeline won't fail afterwards.

\subsection{Property Filter}
Property Filter is an internal transformer for filtering of properties used internally by \odcs from input data. It is implemented in class \code{ODCSPropertyFilterTransformer}.

Property Filter simply removes all triples that have any of the filtered URIs in place of the predicate (see Input Processing in \refusermanual{}) from the \code{payload} and \code{provenance} named graphs.

This transformer is automatically added by Engine as the first transformer of every pipeline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Administration Frontend}
\section{Purpose}
\section{Interface}
\section{Implementation}

\subsection{Database Access Layer}
\FE has a layer for accessing the database based on Spring and its JDBC templates. We chose not to use Hibernate due to integration problems with Wicket and use custom implemenation of business and DAO objects.

Entities retrieved from database are represented by POJOs (Plain Old Java Objects). The code that actually retrieves them is in a DAO class, by convention having suffix \code{Dao} and inheriting from the base class \code{Dao}. DAO objects internally call methods of the Spring's \code{JdbcTemplate} and passes to it a class extending \code{CustomRowMapper} which implements creation of the POJO business object(s) from query results.

The DAO objects can be obtained from an instance of \code{DaoLookupFactory} (available e.g. as a protected member of \code{FrontendPage}). A DAO object can be obtained by calling a \code{getDao()} method which returns an existing DAO object or creates a new one if necessary. Signatures of \code{getDao()} methods are:

\begin{verbatim}
  public <T extends Dao> T getDao(Class<T> daoClass)
  public <T extends Dao> T getDao(Class<T> daoClass, boolean commitable)
\end{verbatim}

We utilize generics in Java to obtain a specific type of a DAO class. In addition, there may be two versions of a DAO class -- one for a read-only view of commited version of an entity and one for the working version visible only to the author (see Section \ref{sec:authorshipAuth}). One can use the second version of the \code{getDao()} method and request either the read-only or commitable version.

Commitable and read-only DAOs are implemented using a custom \code{@CommitableDao} annotation. The read-only version should be annotated with \code{@CommitableDao} having the commitable DAO class as its argument. The commitable version must inherit from the read-only DAO.

Hierarchy of DAO objects is depicted on \figurename~\ref{fig:feDAO}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.05\textwidth]{images/dia-fe-dao.png}
    \caption{Selected DAO classes used in \FE}
	\label{fig:feDAO}
\end{figure}

\subsubsection{Important DAO Classes}
Description of the most important DAO classes follows:

\begin{description}[style=nextline,font=\ttfamily]
	\item[Dao] This is the base class of all DAO classes. It keeps an instance of \code{JdbcTemplate} and provides access to it either directly or through utility methods \code{jdbcQuery()}, \code{jdbcQueryForInt()}, \code{jdbcQueryForList()}, etc. It can also execute code in a Spring transaction with \code{executeInTransaction()} and handles proper recognition of some exceptions thrown by Virtuoso JDBC driver.
	\item[DaoTemplate] This method provides convenience methods for loading of one or more entities from the database. Its \code{getTableName()}, \code{getRowMapper()}, \code{getSelectAndFromClause()}, \code{postLoadAllBy()} and \code{postLoadBy()} methods can be used to customize the loading. Other methods are declared as final.
	\item[DaoForEntityWithSurrogateKey] This DAO is used for working with entities with a primary key. The corresponding business objects must inherit from \code{EntityWithSurrogateKey}. It extends \code{DaoTemplate} with additional methods for loading, deleting and saving an entity by its primary key.
	\item[DaoForAuthorableEntity] This class is intended for entities that can be edited only by their author. It adds an abstract method \code{getAuthorId()}.
	\item[AbstractRuleDao] Base class for (QA, DN, Linker) transformer rules. It provides methods for commiting of changes and disables any delete and update operations.
	\item[XXXRuleDao] Concrete classes inheriting from AbstractRuleDao. It can be used for read-only access to rules (their commited version, respectively). \code{save()} and \code{update()} methods throw an exception. It is annotated with \code{@CommittableDao(XXXRuleUncommittedDao.class)} so that the commitable/editable version can be obtained.

		An instance of this DAO may be obtained by calling e.g.\\
		\code{daoLookupFactory.getDao(XXXRuleDao, false)}.
	\item[XXXRuleUncommittedDao] These classes inherit from \code{XXXRuleDao} and provide the editable and commitable view on transformer rules. Changes may be commited in transaction by calling \code{commitChanges()}.

		An instance of this DAO may be obtained by calling e.g.\\
		\code{daoLookupFactory.getDao(XXXRuleDao, true)}.
\end{description}

\subsection{Authorization}
There are two main scopes of authorization in \odcs -- authorization based on roles and authorization based on the authorship of an entity.

\subsubsection{Roles}
Authorization based on roles recognizes 5 roles: Administrator (ADM), Pipeline Creator (PIC), Ontology Creator (ONC), Data Producer (SCR) and Data Consumer (USR). Their detailed description is given in \refusermanual. Roles can be assigned to users in \FE and a user can have any number of roles.

We use means provided by Wicket to apply authorization by role. Pages and components can be marked with \code{@AuthorizeInstantiation} annotation with enumeration of roles that are required to access the page or component (at least one of the roles from the given list is required). The roles assigned to the currently logged-in user are kept in the session object \code{ODCSWebFrontendSession} which extends Wicket \code{AuthenticatedWebSession} for this purpose.

\subsubsection{Authorship}
\label{sec:authorshipAuth}
Authorization based on authorship is necessary for entities that can be only edited by their author. Rule groups and rules, for example, can be only edited by the user who created them or by user having the role Administrator. To facilitate checking of whether the current user is authorized for entity editing, class \code{LimitedEditingPage} extending \code{FrontendPage} was introduced.

\code{LimitedEditingPage} requires two additinal arguments in its constructor: edited entity ID and a DAO class for the authorable entity (\code{DaoForAuthorableEntity}), which can retrieve author based on entity ID. It then checks whether the current user is authorized using a helper class \code{AuthorizationHelper} and makes this information accessible with protected methods.

Every page that needs information about whether the user is authorized for editation of can call protected methods \code{checkUnauthorizedInstantiation()} to prevent the user from displaying the page or \code{isEditable()} to detect whether the user is authorized for editation.

Because transformer rules and related settings must be commited before the changes are visible to  users not 
authorized for editation (who have a read-only access to rules), there are two versions of rules in the database -- one version is visible for the author and Administrators (these tables have suffix \code{\_UNCOMMITED}) and one version visible for Engine and other users. The proper table version for the current user can be obtained by method \code{getVisibleTableVersion()}.

\code{}

\section{Extending}
\subsection{How to Add a New Page}
\subsection{How to Add a New \DN Template}



\todo{authorization system}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{(Conclusion, evaluation)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{(Testing scenario)}
\todo{Do uzivatelskeho manualu?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{(Related Work)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Future Work}
\todo{plus known bugs?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\chapter{Glossary}
\label{chap:glossary}
\input{appendix-glossary}

\chapter{ODCS ontology}

\chapter{Relational Database Schema}
\label{chap:reldb}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-engine.png}
    \caption{Diagrams of database tables related to Engine}
	\label{fig:dbEngine}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-pipelines.png}
    \caption{Diagrams of database tables related to pipelines}
	\label{fig:dbPipelines}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-fe.png}
    \caption{Diagrams of database tables related to \FE}
	\label{fig:dbFrontend}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-qa.png}
    \caption{Diagrams of database tables related to \QA}
	\label{fig:dbQA}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-dn.png}
    \caption{Diagrams of database tables related to \DN}
	\label{fig:dbDN}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-oi.png}
    \caption{Diagrams of database tables related to \OI}
	\label{fig:dbOI}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-cr.png}
    \caption{Diagrams of database tables related to \CR}
	\label{fig:dbCR}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/db-misc.png}
    \caption{Diagrams of miscellaneous database tables}
	\label{fig:dbMisc}
\end{figure}

\chapter{(\quot{chronologicky popis prubehu praci na projektu})}

\chapter{Project organization}

\begin{itemize}
	\item Team, task division (Developer-Component-Main responsibilities)
	\item Meetings
	\item Release process (+ presentation)
	\item Remarks?
\end{itemize}

\end{document}
